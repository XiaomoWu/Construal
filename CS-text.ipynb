{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "diagnostic-shopper",
   "metadata": {},
   "source": [
    "# Text Concreteness (R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-coordination",
   "metadata": {},
   "source": [
    "## Create `dfm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(spacyr)\n",
    "library(quanteda)\n",
    "\n",
    "spacy_initialize(model='en_core_web_lg',\n",
    "                 save_profile = T)\n",
    "\n",
    "ld(pjson, ldtype='feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "respected-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create corpus\n",
    "corpus = pjson[, .(pid, project_desc)] %>%\n",
    "    corpus(docid_field='pid', text_field='project_desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize with spacy\n",
    "# the results is a data.frame\n",
    "# 1) keep both \"raw\" and \"lemma\" tokens \n",
    "# 2) tokens are case-sensitive\n",
    "tokens_as_df = corpus %>%\n",
    "    spacy_parse(pos=F, entity=F)\n",
    "\n",
    "sv(tokens_as_df)\n",
    "sv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "adjusted-brave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokens consisting of 1 document.\n",
       "1000064918 :\n",
       " [1] \"the\"     \"Beard\"   \"be\"      \"a\"       \"comedy\"  \"base\"    \"comic\"  \n",
       " [8] \"about\"   \"an\"      \"average\" \"guy\"     \"that\"   \n",
       "[ ... and 108 more ]\n",
       "-tokens_lemmatized- saved  (10.24 secs)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert `tokens_as_df` to quanteda `tokens` object\n",
    "# 1) we use the lemmatized tokens, because the lookup table is also lemmatized \n",
    "# 2) tokens are case-sensitive\n",
    "tokens_as_qeda = tokens_as_df %>%\n",
    "    as.tokens(use_lemma=T)\n",
    "\n",
    "tokens_as_qeda[1]\n",
    "\n",
    "sv(tokens_as_qeda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of tokens of each doc\n",
    "ntoken_corpus = ntoken(tokens_as_qeda)\n",
    "ntoken_corpus = data.table(pid=names(ntoken_corpus), ntoken=ntoken_corpus)\n",
    "ntoken_corpus[1]\n",
    "sv(ntoken_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "harmful-charles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_as_qeda (47.5 MB) already loaded, will NOT load again! (0 secs) (2021-03-07 5:55 PM)\n",
      "\"cs_dfm_first200\" saved as \"cs_dfm_first200.rds\" (43.1 MB) (7.55 secs, 2021-03-07 17:56:10)\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to dfm\n",
    "ld(tokens_as_qeda, force=T)\n",
    "\n",
    "tokens_to_dfm <- function(tokens_as_qeda, startpos=1, endpos=-1) {\n",
    "    # select tokens\n",
    "    tokens = tokens_select(tokens_as_qeda, startpos=startpos, endpos=endpos)\n",
    "    \n",
    "    # create ngram\n",
    "    tokens_ngram = tokens %>%\n",
    "        tokens_ngrams(n=1:2, concatenator = \" \")\n",
    "    \n",
    "    # create dfm\n",
    "    cs_dfm = tokens_ngram %>% dfm(tolower=T, stem=F)\n",
    "}\n",
    "\n",
    "cs_dfm = tokens_to_dfm(tokens_as_qeda)\n",
    "cs_dfm_first200 = tokens_to_dfm(tokens_as_qeda, endpos=200)\n",
    "\n",
    "# sv(cs_dfm)\n",
    "sv(cs_dfm_first200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-leeds",
   "metadata": {},
   "source": [
    "## Compute B-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "viral-progress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"df\" saved as \"nltk_stopwords.feather\" (2.5 KB) (<1s)\n"
     ]
    }
   ],
   "source": [
    "# get stopwords from nltk (Python code)\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "df = pd.DataFrame({'word':list(stopwords)})\n",
    "sv('df', svname='nltk_stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "played-devices",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"nltk_stopwords.feather\" (2.5 KB) loaded (0 secs) (2021-03-07 6:00 PM)\n",
      "\"bscore_nostopwords\" saved as \"bscore_nostopwords.rds\" (232.4 KB) (0.04 secs, 2021-03-07 18:00:19)\n",
      "0.310356910447018% words are stopwords"
     ]
    }
   ],
   "source": [
    "# ------------ Create bscore dict ----------------\n",
    "# load stop word list\n",
    "ld(nltk_stopwords, force=T)\n",
    "nltk_stopwords = nltk_stopwords[,word]\n",
    "\n",
    "# read raw bscore\n",
    "bscore_dt = fread('data/concreteness score.csv')[, .(word=str_trim(Word), score=Conc.M)]\n",
    "\n",
    "# create TWO bscore, one has stopwords, one doesn't \n",
    "bscore = bscore_dt$score\n",
    "names(bscore) = bscore_dt$word\n",
    "\n",
    "bscore_nostopwords = bscore[!(names(bscore)%in%nltk_stopwords)]\n",
    "\n",
    "sprintf('%s%% words are stopwords', (1-length(bscore_nostopwords)/length(bscore))*100%>%round(2)) %>% cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "animal-gates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ntoken_corpus.rds\" (282.9 KB) loaded (0.02 secs) (2021-03-08 2:28 AM)\n",
      "cs_dfm (116.8 MB) already loaded, will NOT load again! (0 secs) (2021-03-08 2:28 AM)\n",
      "cs_dfm_first200 (43.1 MB) already loaded, will NOT load again! (0 secs) (2021-03-08 2:28 AM)\n",
      "bscore_nostopwords (232.4 KB) already loaded, will NOT load again! (0 secs) (2021-03-08 2:28 AM)\n"
     ]
    }
   ],
   "source": [
    "# ------------ Create bscore from dtm ----------------\n",
    "\n",
    "ld(ntoken_corpus, force=T)\n",
    "ld(cs_dfm)\n",
    "ld(cs_dfm_first200)\n",
    "ld(bscore_nostopwords)\n",
    "\n",
    "\n",
    "dfm_to_bscore <- function(cs_dfm, bscore_dict, type_name='') {\n",
    "    ntoken_name = str_c('ntoken_bscore', type_name)\n",
    "    bscore_name = str_c('bscore', type_name)\n",
    "    \n",
    "    output_name = c('pid', bscore_name)\n",
    "    \n",
    "    dfm_bscore = dfm_match(cs_dfm, names(bscore_dict))\n",
    "    ntoken_bscore = ntoken(dfm_bscore)\n",
    "    ntoken_bscore_dt = data.table(pid=names(ntoken_bscore))\n",
    "    ntoken_bscore_dt[, (ntoken_name) := ntoken_bscore]\n",
    "    \n",
    "    \n",
    "    dfm_bscore_weighted = dfm_weight(dfm_bscore, weights=bscore_dict)\n",
    "    dfm_bscore_weighted = convert(dfm_bscore_weighted, 'data.frame',\n",
    "                                  docid_field='pid'\n",
    "                                 ) %>% as.data.table()\n",
    "    \n",
    "    bscore_by_pid = dfm_bscore_weighted[, (bscore_name) := rowSums(.SD),\n",
    "                                        .SDcols=is.numeric\n",
    "        ][, ..output_name]\n",
    "    \n",
    "    bscore = bscore_by_pid[ntoken_bscore_dt, on=.(pid)]\n",
    "}\n",
    "\n",
    "bscore_bypid = dfm_to_bscore(cs_dfm, bscore)\n",
    "bscore_bypid_nostopwords = dfm_to_bscore(cs_dfm, bscore_nostopwords, \n",
    "                                         type_name='_nostopwords')\n",
    "bscore_bypid_firstn = dfm_to_bscore(cs_dfm_first200, bscore,\n",
    "                                    type_name='_first200')\n",
    "bscore_bypid_firstn_nostopwords = dfm_to_bscore(cs_dfm_first200,\n",
    "                                                bscore_nostopwords,\n",
    "                                                type_name='_first200_nostopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "general-genetics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"bscore_bypid\" saved as \"bscore_bypid.feather\" (1.7 MB) (0.01 secs, 2021-03-08 02:28:57)\n"
     ]
    }
   ],
   "source": [
    "bscore_bypid = bscore_bypid[bscore_bypid_nostopwords, on=.(pid)\n",
    "    ][bscore_bypid_firstn, on=.(pid)\n",
    "    ][bscore_bypid_firstn_nostopwords, on=.(pid)\n",
    "    ][ntoken_corpus, on=.(pid)]\n",
    "\n",
    "sv(bscore_bypid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "historic-accident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'pid'</li><li>'bscore'</li><li>'ntoken_bscore'</li><li>'bscore_nostopwords'</li><li>'ntoken_bscore_nostopwords'</li><li>'bscore_first200'</li><li>'ntoken_bscore_first200'</li><li>'bscore_first200_nostopwords'</li><li>'ntoken_bscore_first200_nostopwords'</li><li>'ntoken'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'pid'\n",
       "\\item 'bscore'\n",
       "\\item 'ntoken\\_bscore'\n",
       "\\item 'bscore\\_nostopwords'\n",
       "\\item 'ntoken\\_bscore\\_nostopwords'\n",
       "\\item 'bscore\\_first200'\n",
       "\\item 'ntoken\\_bscore\\_first200'\n",
       "\\item 'bscore\\_first200\\_nostopwords'\n",
       "\\item 'ntoken\\_bscore\\_first200\\_nostopwords'\n",
       "\\item 'ntoken'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'pid'\n",
       "2. 'bscore'\n",
       "3. 'ntoken_bscore'\n",
       "4. 'bscore_nostopwords'\n",
       "5. 'ntoken_bscore_nostopwords'\n",
       "6. 'bscore_first200'\n",
       "7. 'ntoken_bscore_first200'\n",
       "8. 'bscore_first200_nostopwords'\n",
       "9. 'ntoken_bscore_first200_nostopwords'\n",
       "10. 'ntoken'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"pid\"                                \"bscore\"                            \n",
       " [3] \"ntoken_bscore\"                      \"bscore_nostopwords\"                \n",
       " [5] \"ntoken_bscore_nostopwords\"          \"bscore_first200\"                   \n",
       " [7] \"ntoken_bscore_first200\"             \"bscore_first200_nostopwords\"       \n",
       " [9] \"ntoken_bscore_first200_nostopwords\" \"ntoken\"                            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names(bscore_bypid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-dubai",
   "metadata": {},
   "source": [
    "# Word Freq (Py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "piano-husband",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "import datatable as dt\n",
    "import pyarrow.feather as feather\n",
    "from datatable import update, f, join, by\n",
    "\n",
    "tokens_as_df = feather.read_feather('data/tokens_as_df.feather',\n",
    "                                    columns=['doc_id', 'token'])\n",
    "tokens_as_df['word'] = tokens_as_df.token.str.lower()\n",
    "tokens_as_df = dt.Frame(tokens_as_df)[:,[f.doc_id, f.word]]\n",
    "\n",
    "freqdict = dt.fread('data/freqdict.csv')\n",
    "freqdict.key = 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "further-arnold",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute freq\n",
    "result = tokens_as_df[:,:,join(freqdict)\n",
    "    ][:,{'score':dt.sum(f.freq),'n_words':dt.count()}, by(f.doc_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "challenging-protocol",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='datatable'>\n",
       "  <table class='frame'>\n",
       "  <thead>\n",
       "    <tr class='colnames'><td class='row_index'></td><th>doc_id</th><th>score</th><th>n_words</th></tr>\n",
       "    <tr class='coltypes'><td class='row_index'></td><td class='str' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='int' title='int64'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><td class='row_index'>0</td><td>1000064918</td><td>16.5642</td><td>120</td></tr>\n",
       "    <tr><td class='row_index'>1</td><td>1000081649</td><td>51.0863</td><td>407</td></tr>\n",
       "    <tr><td class='row_index'>2</td><td>1000103948</td><td>112.149</td><td>773</td></tr>\n",
       "    <tr><td class='row_index'>3</td><td>1000117510</td><td>21.1289</td><td>335</td></tr>\n",
       "    <tr><td class='row_index'>4</td><td>1000201265</td><td>129.156</td><td>1013</td></tr>\n",
       "    <tr><td class='row_index'>5</td><td>1000234595</td><td>99.2084</td><td>768</td></tr>\n",
       "    <tr><td class='row_index'>6</td><td>1000291263</td><td>61.2186</td><td>471</td></tr>\n",
       "    <tr><td class='row_index'>7</td><td>1000335422</td><td>156.628</td><td>1438</td></tr>\n",
       "    <tr><td class='row_index'>8</td><td>1000426032</td><td>108.298</td><td>959</td></tr>\n",
       "    <tr><td class='row_index'>9</td><td>1000450327</td><td>65.6153</td><td>668</td></tr>\n",
       "    <tr><td class='row_index'>10</td><td>1000491057</td><td>42.8523</td><td>332</td></tr>\n",
       "    <tr><td class='row_index'>11</td><td>1000537760</td><td>21.7049</td><td>191</td></tr>\n",
       "    <tr><td class='row_index'>12</td><td>100057323</td><td>93.9927</td><td>862</td></tr>\n",
       "    <tr><td class='row_index'>13</td><td>10005784</td><td>89.6294</td><td>750</td></tr>\n",
       "    <tr><td class='row_index'>14</td><td>1000581804</td><td>14.128</td><td>127</td></tr>\n",
       "    <tr><td class='row_index'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td><td class='hellipsis'>&#x22EE;</td></tr>\n",
       "    <tr><td class='row_index'>42,908</td><td>999701844</td><td>12.4317</td><td>120</td></tr>\n",
       "    <tr><td class='row_index'>42,909</td><td>999715332</td><td>31.4234</td><td>339</td></tr>\n",
       "    <tr><td class='row_index'>42,910</td><td>999829547</td><td>53.4627</td><td>514</td></tr>\n",
       "    <tr><td class='row_index'>42,911</td><td>99984851</td><td>22.3248</td><td>243</td></tr>\n",
       "    <tr><td class='row_index'>42,912</td><td>999884872</td><td>267.216</td><td>2289</td></tr>\n",
       "  </tbody>\n",
       "  </table>\n",
       "  <div class='footer'>\n",
       "    <div class='frame_dimensions'>42,913 rows &times; 3 columns</div>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<Frame#7fea783ff230 42913x3>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n",
    "feather.write_feather(result.to_pandas(), 'data/result.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
