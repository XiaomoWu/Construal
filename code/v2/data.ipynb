{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared\n",
    "\n",
    "The code in this block will be used to compute uniqueness, readability, and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from pyarrow.feather import write_feather, read_feather\n",
    "from tqdm import tqdm\n",
    "\n",
    "wdir = Path('/home/yu/OneDrive/Construal/')\n",
    "os.chdir(wdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the V3D category list table (i.e., `cat_info`)\n",
    "- Category ID\n",
    "- Category name\n",
    "- Category description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category info of train\n",
    "# train and val are the same, so we only use train\n",
    "with open('pretrained-models/v3det/data/V3Det/annotations/v3det_2023_v1_train.json') as f:\n",
    "    cat_info = json.load(f)\n",
    "    cat_info = pd.DataFrame(cat_info['categories'])\n",
    "\n",
    "    write_feather(cat_info, 'data/v2/v3det/category_info.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect object detetion results on kickstarter projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:12<00:00, 301.43it/s] \n"
     ]
    }
   ],
   "source": [
    "# get file paths of object detection results\n",
    "p = wdir/'data/v2/v3det/on-kickstarter/per-image-results/'\n",
    "files = list(p.glob('*.pt'))\n",
    "\n",
    "# img root directory\n",
    "img_root = Path('/home/yu/chaoyang/research-resources/kickstart-raw-from-amrita/kickstarter-image')\n",
    "\n",
    "# loop over every image results\n",
    "obj_det_results = []\n",
    "for i, f in enumerate(tqdm(files)):\n",
    "    # load the results\n",
    "    df = torch.load(f)\n",
    "\n",
    "    # get size of each object\n",
    "    obj_size = [w*h for x, y, w, h in df['bboxes']]\n",
    "\n",
    "    # get the image path\n",
    "    img_path = img_root/f.stem/'profile_full.jpg'\n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "\n",
    "    # get the image size\n",
    "    with Image.open(img_path) as img:\n",
    "        w, h = img.size\n",
    "        img_size = w * h\n",
    "\n",
    "    # get the ratio of each object\n",
    "    obj_size_ratio = [x/img_size for x in obj_size]\n",
    "    \n",
    "    # collect the results into a dataframe\n",
    "    df = pd.DataFrame(\n",
    "        {'pid': f.stem, 'label': df['labels'], 'score': df['scores'], \n",
    "         'size_ratio': obj_size_ratio})\n",
    "    obj_det_results.append(df)\n",
    "\n",
    "# covert to dataframe\n",
    "obj_det_results = pd.concat(obj_det_results, ignore_index=True)\n",
    "obj_det_results['label'] = obj_det_results['label'].astype(int) + 1\n",
    "write_feather(obj_det_results, 'data/v2/v3det/on-kickstarter/obj_det_results.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniqueness (frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get object-level freq (Kickstarter)\n",
    "\n",
    "Compute object frequency where the context is limited two kickstarter product categories: \"product design\" and \"accessories.\"\n",
    "\n",
    "**Attention!**\n",
    "\n",
    "The label IDs outputed by MMdet are indexed from 0 (i.e., 0, 1, 2...) but the official category list is indexed from 1. To align the two, we need to add one to each of the predicted ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages({\n",
    "    library(arrow)\n",
    "})\n",
    "\n",
    "wdir = '/home/yu/OneDrive/Construal/'\n",
    "setwd(wdir)\n",
    "\n",
    "# we set the threshold of probability to be 0.1 or 0.5 (we tried two versions)\n",
    "score_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Number of object categories: 194\"\n",
      "[1] \"Number of projects: 873\"\n"
     ]
    }
   ],
   "source": [
    "# read detection results\n",
    "obj_det_results = read_feather(\n",
    "    'data/v2/v3det/on-kickstarter/obj_det_results.feather',\n",
    "    col_select=c('pid', 'label', 'score')) %>% setDT()\n",
    "\n",
    "# we only keep the results with score >= score_threshold\n",
    "# and then compute frequency\n",
    "freq = obj_det_results[\n",
    "    score >= score_threshold,\n",
    "    .(freq=.N),\n",
    "    keyby=.(label)\n",
    "    # normalize the largest freq to 1\n",
    "    ][, ':='(freq=freq/max(freq))]\n",
    "\n",
    "# print number of object categories\n",
    "print(sprintf('Number of object categories: %d', nrow(freq)))\n",
    "\n",
    "# print number of projects\n",
    "print(sprintf('Number of projects: %d', obj_det_results[score>=score_threshold, uniqueN(pid)]))\n",
    "\n",
    "# add category name\n",
    "cat_info = read_feather('data/v2/v3det/category_info.feather') %>% setDT()\n",
    "\n",
    "freq = freq[\n",
    "    cat_info[, .(id, name)], \n",
    "    on=c('label==id'), nomatch=NULL\n",
    "    ][order(-freq)]\n",
    "\n",
    "# save the frequency table\n",
    "write_feather(freq, sprintf('data/v2/v3det/on-kickstarter/freq_p%s.feather', score_threshold*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get object-level frequency (V3D)\n",
    "\n",
    "Compute object frequency where the context is the whole training dataset of V3D.\n",
    "\n",
    "**Attention!**\n",
    "\n",
    "The label IDs outputed by MMdet are indexed from 0 (i.e., 0, 1, 2...) but the official category list is indexed from 1. To align the two, we need to add one to each of the predicted ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Number of object categories: 12913\"\n",
      "[1] \"Number of projects: 209308\"\n"
     ]
    }
   ],
   "source": [
    "suppressMessages({\n",
    "    library(arrow)\n",
    "})\n",
    "\n",
    "wdir = '/home/yu/OneDrive/Construal/'\n",
    "setwd(wdir)\n",
    "\n",
    "# we set the threshold of probability (for V3D training datset) to be 0.5 \n",
    "# (we don't use 0.1 becaue using 0.5 will still give us enough data)\n",
    "score_threshold = 0.5\n",
    "\n",
    "# read detection results\n",
    "obj_det_results = read_feather(\n",
    "    'data/v2/v3det/on-train/obj_det_results.feather',\n",
    "    col_select=c('img_name', 'label', 'score')) %>% setDT()\n",
    "\n",
    "# we set the score threashold to 0.5 (typicall value is 0.5 to 0.95) and compute frequency\n",
    "freq = obj_det_results[\n",
    "    score >= score_threshold,\n",
    "    .(freq=.N),\n",
    "    keyby=.(label)\n",
    "    # normalize the largest freq to 1\n",
    "    ][, ':='(freq=freq/max(freq))]\n",
    "\n",
    "# print number of object categories\n",
    "print(sprintf('Number of object categories: %d', nrow(freq)))\n",
    "\n",
    "# print number of images left\n",
    "print(sprintf('Number of projects: %d', obj_det_results[score>=score_threshold, uniqueN(img_name)]))\n",
    "\n",
    "# add category name\n",
    "cat_info = read_feather('data/v2/v3det/category_info.feather') %>% setDT()\n",
    "\n",
    "freq = freq[\n",
    "    cat_info[, .(id, name)], \n",
    "    on=c('label==id'), nomatch=NULL\n",
    "    ][order(-freq)]\n",
    "\n",
    "# save the frequency table\n",
    "write_feather(freq, sprintf('data/v2/v3det/on-train/freq_p%s.feather', score_threshold*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get project-level frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Number of projects: 3704\"\n"
     ]
    }
   ],
   "source": [
    "# we set the threshold of probability to be 0.1 or 0.5\n",
    "score_threshold = 0.1\n",
    "\n",
    "# load the object-level frequency table (Kick- and V3D-context)\n",
    "freq_kick = read_feather(\n",
    "    sprintf('data/v2/v3det/on-kickstarter/freq_p%s.feather', score_threshold*100), \n",
    "    col_select=c('label', 'freq')) %>% setDT()\n",
    "\n",
    "freq_v3d = read_feather(\n",
    "    # for v3d, we only use one threahold (0.5)\n",
    "    'data/v2/v3det/on-train/freq_p50.feather', \n",
    "    col_select=c('label', 'freq')) %>% setDT()\n",
    "\n",
    "setnames(freq_kick, 'freq', 'freq_kick')\n",
    "setnames(freq_v3d, 'freq', 'freq_v3d')\n",
    "\n",
    "# combine the two frequency tables into one, `freq`\n",
    "freq = freq_kick[freq_v3d, on=c('label'), nomatch=NULL]\n",
    "\n",
    "# load the object detection results\n",
    "obj_det_results = read_feather('data/v2/v3det/on-kickstarter/obj_det_results.feather') %>% setDT()\n",
    "\n",
    "# we only keep objects with score >= score_threshold\n",
    "obj_det_results = obj_det_results[score>=score_threshold]\n",
    "\n",
    "# compute the freq of each project\n",
    "# by aggregating the freq of each object in the project\n",
    "proj_freq = obj_det_results[\n",
    "    freq, on=.(label), nomatch=NULL\n",
    "    # two versions: simple average and weighted avg by probability\n",
    "    ][, .(freq_kick=sum(freq_kick), freq_kick_w=sum(freq_kick*score),\n",
    "          freq_v3d=sum(freq_v3d), freq_v3d_w=sum(freq_v3d*score)),\n",
    "    keyby=.(pid)]\n",
    "\n",
    "# print the number of projects left\n",
    "print(sprintf('Number of projects: %d', uniqueN(proj_freq$pid)))\n",
    "\n",
    "write_feather(proj_freq, sprintf('data/v2/proj_freq_p%s.feather', score_threshold*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability\n",
    "\n",
    "Compute the 1) number of objects, and 2) size (area) of each object\n",
    "\n",
    "Unlike \"uniqueness,\" readability does not differentiate between V3D context or Kick context because the computation purely relies on the attribute of the image itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages({\n",
    "    library(arrow)\n",
    "})\n",
    "\n",
    "wdir = '/home/yu/OneDrive/Construal/'\n",
    "setwd(wdir)\n",
    "\n",
    "# we set the threshold of probability to be 0.1 (0.5 will remove too many images)\n",
    "score_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Number of projects: 873\"\n"
     ]
    }
   ],
   "source": [
    "# read obj detection results of kickstart projects\n",
    "obj_det_results = read_feather(\n",
    "    'data/v2/v3det/on-kickstarter/obj_det_results.feather') %>% setDT()\n",
    "\n",
    "# we only keep the results with score >= score_threshold\n",
    "# and then compute frequency\n",
    "readability = obj_det_results[\n",
    "    score >= score_threshold\n",
    "    ][, .(\n",
    "    # compute object number\n",
    "      obj_num=.N, obj_num_w=sum(score),\n",
    "    # compute object size\n",
    "      obj_size_lt_5=sum(size_ratio<=0.05),\n",
    "      obj_size_lt_10=sum(size_ratio<=0.1),\n",
    "      obj_size_lt_20=sum(size_ratio<=0.2), \n",
    "      obj_size_lt_50=sum(size_ratio<=0.5),\n",
    "      obj_size_w_lt_5=sum((score*size_ratio)<=0.05),\n",
    "      obj_size_w_lt_10=sum((score*size_ratio)<=0.1),\n",
    "      obj_size_w_lt_20=sum((score*size_ratio)<=0.2),\n",
    "      obj_size_w_lt_50=sum((score*size_ratio)<=0.5)\n",
    "    ), \n",
    "    keyby=.(pid)]\n",
    "\n",
    "# print number of projects\n",
    "print(sprintf('Number of projects: %d', uniqueN(readability$pid)))\n",
    "\n",
    "# save the readability table\n",
    "write_feather(readability, sprintf('data/v2/proj_read_p%s.feather', score_threshold*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNI concreteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from IPython.utils import io\n",
    "from pathlib import Path\n",
    "from pyarrow.feather import write_feather, read_feather\n",
    "from tqdm import tqdm\n",
    "\n",
    "wdir = Path('/home/yu/OneDrive/Construal')\n",
    "os.chdir(wdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Per-image Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Get the embeddings of images. It's done by `v3d-img-embed-on-kickstarter.py` and `v3d-img-embed-on-train.py`.\n",
    "\n",
    "The output is stored at `data/v2/v3det/on-kickstarter/per-image-embed` and `data/v2/v3det/on-train/per-image-embed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect Results into a Dict (Kickstarter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every image in the Kickstarter has its own embedding stored as a `.pt` file. We collect and combine them into a single dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3749 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3749/3749 [00:00<00:00, 8558.85it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_root = Path('data/v2/v3det/on-kickstarter/per-image-embed')\n",
    "emb_paths = list(emb_root.glob('*.pt'))\n",
    "\n",
    "# initialize a dictionary to store embeddings\n",
    "# key: pid, value: embedding tensor\n",
    "emb_dict = {}\n",
    "\n",
    "# load embeddings\n",
    "for i, path in enumerate(tqdm(emb_paths)):\n",
    "    emb_dict[path.stem] = torch.load(path)\n",
    "\n",
    "# save embeddings\n",
    "torch.save(emb_dict, 'data/v2/v3det/on-kickstarter/img_emb_results.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect Results into a Dict (V3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every image in the training set of V3D has its own embedding stored as a `.pt` file. We collect and combine them into a single dictory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213055/213055 [00:22<00:00, 9338.61it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_root = Path('data/v2/v3det/on-train/per-image-embed')\n",
    "emb_paths = list(emb_root.glob('*.pt'))\n",
    "\n",
    "# initialize a dictionary to store embeddings\n",
    "# key: pid, value: embedding tensor\n",
    "emb_dict = {}\n",
    "\n",
    "# load embeddings\n",
    "for i, path in enumerate(tqdm(emb_paths)):\n",
    "    emb_dict[path.stem] = torch.load(path)\n",
    "\n",
    "# save embeddings\n",
    "torch.save(emb_dict, 'data/v2/v3det/on-train/img_emb_results.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get object-level MNI (Kickstarter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To compute MNI, we first compute the MNI score of every **object category**, $MNI_{obj}$. \n",
    "\n",
    "We only use the **Kickstarter images** (i.e., context dependant) in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Build annoy tree --- #\n",
    "\n",
    "# Load image embeddings into an annoy tree\n",
    "embeds = torch.load(f'{wdir}/data/v2/v3det/on-kickstarter/img_emb_results.pt')\n",
    "valid_pids = list(embeds.keys())\n",
    "\n",
    "# load image reprs into an annoy tree\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "# initialize annoy tree \"t\"\n",
    "t = AnnoyIndex(1024, 'angular')\n",
    "\n",
    "# a map from pid to an int index\n",
    "img2id = {}\n",
    "\n",
    "for i, (img, vec) in enumerate(embeds.items()): \n",
    "\n",
    "    # create a map from pid to an int index\n",
    "    img2id[img] = i\n",
    "\n",
    "    # add image embeddings to annoy tree\n",
    "    t.add_item(i, vec)\n",
    "    \n",
    "# build annoy tree\n",
    "\n",
    "# set seed\n",
    "t.set_seed(42)\n",
    "\n",
    "# n_trees is the number of hyperplans splits we want to build\n",
    "# 10 to 100 is a good starting point of n_trees, but since we only have 3k images \n",
    "# in kickstarter, we can build more trees (i.e., 1000)\n",
    "t.build(n_trees=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique objects: 194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/194 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:01<00:00, 127.00it/s]\n",
      "100%|██████████| 194/194 [00:02<00:00, 96.35it/s] \n",
      "100%|██████████| 194/194 [00:02<00:00, 69.36it/s] \n",
      "100%|██████████| 194/194 [00:04<00:00, 45.44it/s] \n"
     ]
    }
   ],
   "source": [
    "# --- Compute MNI (for different K neighbors) --- #\n",
    "\n",
    "# we set the threshold of probability to be 0.1 or 0.5\n",
    "score_threshold = 0.5\n",
    "\n",
    "\n",
    "# load object detection results\n",
    "obj_det_results = read_feather(wdir/'data/v2/v3det/on-kickstarter/obj_det_results.feather')\n",
    "\n",
    "# we only keep objects with probability >= 0.1 (0.5 only leaves us about 870)\n",
    "# this gives us about 3k object categories\n",
    "obj_det_results = obj_det_results[obj_det_results.score>=score_threshold]\n",
    "\n",
    "# get a list of all unique objects, `objs`\n",
    "# each element in `objs` is an integer ID for an object category\n",
    "objs = obj_det_results.label.unique().tolist()\n",
    "print(f'Number of unique objects: {len(objs)}')\n",
    "\n",
    "# get total number of images \n",
    "# the `V` in the formula, which is used to normalize MNI\n",
    "V = len(valid_pids)\n",
    "\n",
    "# main function to compute MNI\n",
    "def make_mni(k, t, search_k=-1):\n",
    "    '''\n",
    "    Args:\n",
    "        k: number of nearest neighbors\n",
    "        t: a compiled annoy tree\n",
    "        search_k: number of nodes to search (see doc of annoy)\n",
    "            default search_k = -1 (full search, = n_trees * k)\n",
    "    '''\n",
    "    \n",
    "    # initialize a dictionary to store mni\n",
    "    mni_dict = {}\n",
    "\n",
    "    # compute mni for each object category\n",
    "    for i, obj in enumerate(tqdm(objs)):\n",
    "\n",
    "        # `obj` is an integer ID for an object category\n",
    "\n",
    "        # get a list of all images (named by its pid) that contain `obj`\n",
    "        V_obj = obj_det_results[obj_det_results.label==obj].pid.unique().tolist()\n",
    "\n",
    "        # convert this list of pids to a list of int indices\n",
    "        # remember that `pid2id` is a map from pid to an int index\n",
    "        V_obj = [img2id[pid] for pid in V_obj]\n",
    "        V_obj = set(V_obj)\n",
    "        \n",
    "        # compute mni\n",
    "        a = 0\n",
    "        for v in V_obj:\n",
    "            # `v` is an int index for an image\n",
    "\n",
    "            # get a list of k nearest neighbors (named by its int indices) \n",
    "            # for `v` (excluding `v` itself)\n",
    "            NN_v = set(t.get_nns_by_item(v, k, search_k=search_k)) - set([v])\n",
    "\n",
    "            # get the number of images that contain `obj` and are also in `NN_v`\n",
    "            a += len(V_obj.intersection(NN_v))\n",
    "\n",
    "        # divide by the total number of images that contain `obj`\n",
    "        mni_obj = a/len(V_obj)\n",
    "\n",
    "        # normalize mni\n",
    "        adj_mni = mni_obj / (len(V_obj)*k) * V\n",
    "\n",
    "        mni_dict[obj] = adj_mni\n",
    "\n",
    "    df = pd.DataFrame(mni_dict.items(), columns=['obj', 'mni'])\n",
    "\n",
    "    return df\n",
    "\n",
    "obj_mni_k10 = make_mni(10, t)\n",
    "obj_mni_k25 = make_mni(25, t)\n",
    "obj_mni_k50 = make_mni(50, t)\n",
    "obj_mni_k100 = make_mni(100, t)\n",
    "\n",
    "obj_mni_k10.to_feather(wdir/f'data/v2/v3det/on-kickstarter/obj_mni_k10_p{int(score_threshold*100)}.feather')\n",
    "obj_mni_k25.to_feather(wdir/f'data/v2/v3det/on-kickstarter/obj_mni_k25_p{int(score_threshold*100)}.feather')\n",
    "obj_mni_k50.to_feather(wdir/f'data/v2/v3det/on-kickstarter/obj_mni_k50_p{int(score_threshold*100)}.feather')\n",
    "obj_mni_k100.to_feather(wdir/f'data/v2/v3det/on-kickstarter/obj_mni_k100_p{int(score_threshold*100)}.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get object-level MNI (V3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To compute MNI, we first compute the MNI score of every **object category**, $MNI_{obj}$. \n",
    "\n",
    "We only use the **V3D training data** (i.e., context independant) in this section.\n",
    "\n",
    "In the next section, we then compute the MNI of every project by aggregating the MNI \n",
    "scores of its containing objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213055/213055 [00:18<00:00, 11776.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Build annoy tree --- #\n",
    "\n",
    "# Load image embeddings into an annoy tree\n",
    "embeds = torch.load(f'{wdir}/data/v2/v3det/on-train/img_emb_results.pt')\n",
    "\n",
    "# get valid image names\n",
    "valid_imgs = list(embeds.keys())\n",
    "\n",
    "# load image reprs into an annoy tree\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "# initialize annoy tree \"t\"\n",
    "t = AnnoyIndex(1024, 'angular')\n",
    "\n",
    "# a map from image name to an int index\n",
    "img2id = {}\n",
    "\n",
    "# add embeddings to annoy tree\n",
    "for i, (img, vec) in enumerate(tqdm(embeds.items())): \n",
    "\n",
    "    # img is a string of the image name\n",
    "\n",
    "    # create a map from pid to an int index\n",
    "    img2id[img] = i\n",
    "\n",
    "    # add image embeddings to annoy tree\n",
    "    t.add_item(i, vec)\n",
    "    \n",
    "# build annoy tree\n",
    "# if search_k is fixed, then n_trees won't affect query time\n",
    "# 100 trees took about 35s to build\n",
    "t.build(n_trees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute MNI (for different K neighbors) --- #\n",
    "\n",
    "# we set the threshold of probability to be 0.5 (for V3D training dataset we only use 0.5)\n",
    "score_threshold = 0.5\n",
    "\n",
    "\n",
    "# load object detection results\n",
    "obj_det_results = read_feather(wdir/'data/v2/v3det/on-train/obj_det_results.feather')\n",
    "\n",
    "# we only keep objects with probability >= 0.1 (0.5 only leaves us about 870)\n",
    "# this gives us about 3k object categories\n",
    "obj_det_results = obj_det_results[obj_det_results.score>=score_threshold]\n",
    "\n",
    "# get a list of all unique objects, `objs`\n",
    "# each element in `objs` is an integer ID for an object category\n",
    "objs = obj_det_results.label.unique().tolist()\n",
    "\n",
    "# get total number of images \n",
    "# the `V` in the formula, which is used to normalize MNI\n",
    "V = len(valid_imgs)\n",
    "\n",
    "# print the number of unique objects and images\n",
    "print(f'Number of unique objects: {len(objs)}')\n",
    "print(f'Number of images: {V}')\n",
    "\n",
    "# main function to compute MNI\n",
    "def make_mni(k, t, search_k=-1):\n",
    "    '''\n",
    "    Args:\n",
    "        k: number of nearest neighbors\n",
    "        t: a compiled annoy tree\n",
    "        search_k: number of nodes to search (see doc of annoy)\n",
    "    '''\n",
    "    \n",
    "    # initialize a dictionary to store mni\n",
    "    mni_dict = {}\n",
    "\n",
    "    # compute mni for each object category\n",
    "    for i, obj in enumerate(tqdm(objs)):\n",
    "\n",
    "        # `obj` is an integer ID for an object category\n",
    "\n",
    "        # get a list of all images (named by its img_name) that contain `obj`\n",
    "        V_obj = obj_det_results[obj_det_results.label==obj].img_name.unique().tolist()\n",
    "\n",
    "        # convert this list of pids to a list of int indices\n",
    "        # remember that `pid2id` is a map from pid to an int index\n",
    "        V_obj = [img2id[pid] for pid in V_obj]\n",
    "        V_obj = set(V_obj)\n",
    "        \n",
    "        # compute mni\n",
    "        a = 0\n",
    "        for v in V_obj:\n",
    "            # `v` is an int index for an image\n",
    "\n",
    "            # get a list of k nearest neighbors (named by its int indices) \n",
    "            # for `v` (excluding `v` itself)\n",
    "            NN_v = set(t.get_nns_by_item(v, k, search_k=search_k)) - set([v])\n",
    "\n",
    "            # get the number of images that contain `obj` and are also in `NN_v`\n",
    "            a += len(V_obj.intersection(NN_v))\n",
    "\n",
    "        # divide by the total number of images that contain `obj`\n",
    "        mni_obj = a/len(V_obj)\n",
    "\n",
    "        # normalize mni\n",
    "        adj_mni = mni_obj / (len(V_obj)*k) * V\n",
    "\n",
    "        mni_dict[obj] = adj_mni\n",
    "\n",
    "    df = pd.DataFrame(mni_dict.items(), columns=['obj', 'mni'])\n",
    "\n",
    "    return df\n",
    "\n",
    "obj_mni_k10 = make_mni(10, t)\n",
    "obj_mni_k25 = make_mni(25, t)\n",
    "obj_mni_k50 = make_mni(50, t)\n",
    "obj_mni_k100 = make_mni(100, t)\n",
    "\n",
    "obj_mni_k10.to_feather(wdir/f'data/v2/v3det/on-train/obj_mni_k10_p{int(score_threshold*100)}.feather')\n",
    "obj_mni_k25.to_feather(wdir/f'data/v2/v3det/on-train/obj_mni_k25_p{int(score_threshold*100)}.feather')\n",
    "obj_mni_k50.to_feather(wdir/f'data/v2/v3det/on-train/obj_mni_k50_p{int(score_threshold*100)}.feather')\n",
    "obj_mni_k100.to_feather(wdir/f'data/v2/v3det/on-train/obj_mni_k100_p{int(score_threshold*100)}.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get project-level MNI\n",
    "\n",
    "In the previous section, we've computed $MNI_obj$ for each object. Now we compute the MNI for the whole image (project). The project-level MNI is an aggregation of all the MNIs of the containing objects in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages({\n",
    "    library(arrow)\n",
    "})\n",
    "wdir = '/home/yu/OneDrive/Construal'\n",
    "setwd(wdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set the threshold of probability to be 0.1 or 0.5\n",
    "score_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Number of unique objects: 194\"\n"
     ]
    }
   ],
   "source": [
    "# -- load all object-level MNI into one dataset --\n",
    "\n",
    "# load MNI based on kickstarter\n",
    "obj_mni_k10_kick = read_feather(sprintf(\"data/v2/v3det/on-kickstarter/obj_mni_k10_p%s.feather\", score_threshold*100)) %>% setDT()\n",
    "obj_mni_k25_kick = read_feather(sprintf(\"data/v2/v3det/on-kickstarter/obj_mni_k25_p%s.feather\", score_threshold*100)) %>% setDT()\n",
    "obj_mni_k50_kick = read_feather(sprintf(\"data/v2/v3det/on-kickstarter/obj_mni_k50_p%s.feather\", score_threshold*100)) %>% setDT()\n",
    "obj_mni_k100_kick = read_feather(sprintf(\"data/v2/v3det/on-kickstarter/obj_mni_k100_p%s.feather\", score_threshold*100)) %>% setDT()\n",
    "\n",
    "# load MNI based on V3D (for V3D we only use one threshold, 0.5)\n",
    "obj_mni_k10_v3d = read_feather(\"data/v2/v3det/on-train/obj_mni_k10_p50.feather\") %>% setDT()\n",
    "obj_mni_k25_v3d = read_feather(\"data/v2/v3det/on-train/obj_mni_k25_p50.feather\") %>% setDT()\n",
    "obj_mni_k50_v3d = read_feather(\"data/v2/v3det/on-train/obj_mni_k50_p50.feather\") %>% setDT()\n",
    "obj_mni_k100_v3d = read_feather(\"data/v2/v3det/on-train/obj_mni_k100_p50.feather\") %>% setDT()\n",
    "\n",
    "setnames(obj_mni_k10_kick, 'mni', 'mni_k10_kick')\n",
    "setnames(obj_mni_k25_kick, 'mni', 'mni_k25_kick')\n",
    "setnames(obj_mni_k50_kick, 'mni', 'mni_k50_kick')\n",
    "setnames(obj_mni_k100_kick, \"mni\", \"mni_k100_kick\")\n",
    "\n",
    "setnames(obj_mni_k10_v3d, \"mni\", \"mni_k10_v3d\")\n",
    "setnames(obj_mni_k25_v3d, \"mni\", \"mni_k25_v3d\")\n",
    "setnames(obj_mni_k50_v3d, \"mni\", \"mni_k50_v3d\")\n",
    "setnames(obj_mni_k100_v3d, \"mni\", \"mni_k100_v3d\")\n",
    "\n",
    "# merge all MNI datasets\n",
    "obj_mni = obj_mni_k10_kick[\n",
    "    obj_mni_k25_kick, on=.(obj), nomatch=NULL\n",
    "    ][obj_mni_k50_kick, on=.(obj), nomatch=NULL\n",
    "    ][obj_mni_k100_kick, on=.(obj), nomatch=NULL\n",
    "    ][obj_mni_k10_v3d, on=.(obj), nomatch=NULL\n",
    "    ][obj_mni_k25_v3d, on=.(obj), nomatch=NULL\n",
    "    ][obj_mni_k50_v3d, on=.(obj), nomatch=NULL\n",
    "    ][obj_mni_k100_v3d, on=.(obj), nomatch=NULL]\n",
    "\n",
    "# print the number of unique objects\n",
    "sprintf(\"Number of unique objects: %d\", obj_mni[, uniqueN(obj)]) %>% print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Number of unique projects: 873\"\n"
     ]
    }
   ],
   "source": [
    "# --- compute project-level MNI --- #\n",
    "\n",
    "# read detection results\n",
    "obj_det_results = read_feather(\n",
    "    'data/v2/v3det/on-kickstarter/obj_det_results.feather',\n",
    "    col_select=c('pid', 'label', 'score')) %>% setDT()\n",
    "\n",
    "obj_det_results = obj_det_results[score>=score_threshold]\n",
    "\n",
    "# compute project-level MNI\n",
    "proj_mni = obj_det_results[\n",
    "    # add object-level MNI\n",
    "    obj_mni, on=c('label==obj'), nomatch=NULL\n",
    "    # compute project-level MNI\n",
    "    ][, .(\n",
    "        # kick-context\n",
    "        mni_k10_kick=sum(mni_k10_kick), mni_k25_kick=sum(mni_k25_kick),\n",
    "        mni_k50_kick=sum(mni_k50_kick), mni_k100_kick=sum(mni_k100_kick),\n",
    "        mni_k10_kick_w=sum(mni_k10_kick*score), mni_k25_kick_w=sum(mni_k25_kick*score),\n",
    "        mni_k50_kick_w=sum(mni_k50_kick*score), mni_k100_kick_w=sum(mni_k100_kick*score),\n",
    "        # v3d-context\n",
    "        mni_k10_v3d=sum(mni_k10_v3d), mni_k25_v3d=sum(mni_k25_v3d),\n",
    "        mni_k50_v3d=sum(mni_k50_v3d), mni_k100_v3d=sum(mni_k100_v3d),\n",
    "        mni_k10_v3d_w=sum(mni_k10_v3d*score), mni_k25_v3d_w=sum(mni_k25_v3d*score),\n",
    "        mni_k50_v3d_w=sum(mni_k50_v3d*score), mni_k100_v3d_w=sum(mni_k100_v3d*score)\n",
    "    ),\n",
    "    keyby=.(pid)]\n",
    "\n",
    "# save the project-level MNI\n",
    "write_feather(proj_mni, sprintf('data/v2/proj_mni_p%s.feather', score_threshold*100))\n",
    "\n",
    "# print the number of unique projects\n",
    "sprintf(\"Number of unique projects: %d\", proj_mni[, uniqueN(pid)]) %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages({\n",
    "    library(arrow)\n",
    "})\n",
    "wdir = '/home/yu/OneDrive/Construal'\n",
    "setwd(wdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Number of unique projects: 3704\"\n",
      "[1] \"Number of unique projects: 873\"\n"
     ]
    }
   ],
   "source": [
    "combine_all <- function(score_threshold) {\n",
    "    # Args:\n",
    "    #    score_threshold: the threshold of probability (0.1 or 0.5)\n",
    "\n",
    "    # load mni, frequency, and readability\n",
    "    proj_mni = read_feather(sprintf('data/v2/proj_mni_p%s.feather', score_threshold*100)) %>% setDT()\n",
    "    proj_freq = read_feather(sprintf('data/v2/proj_freq_p%s.feather', score_threshold*100)) %>% setDT()\n",
    "    proj_read = read_feather(sprintf('data/v2/proj_read_p%s.feather', score_threshold*100)) %>% setDT()\n",
    "\n",
    "    # merge them into one\n",
    "    proj_metrics = proj_mni[\n",
    "        proj_freq, on=.(pid), nomatch=NULL\n",
    "        ][proj_read, on=.(pid), nomatch=NULL]\n",
    "\n",
    "    # print the number of unique projects\n",
    "    sprintf(\"Number of unique projects: %d\", proj_metrics[, uniqueN(pid)]) %>% print()\n",
    "\n",
    "    # save the dataset\n",
    "    write_feather(proj_metrics, sprintf('data/v2/proj_metrics_p%s.feather', score_threshold*100))\n",
    "\n",
    "    # print one row\n",
    "    proj_metrics[1]\n",
    "\n",
    "}\n",
    "\n",
    "proj_metrics_p10 = combine_all(0.1)\n",
    "proj_metrics_p10 = combine_all(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>730616508</td>\n",
       "      <td>Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1651651025</td>\n",
       "      <td>Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62612974</td>\n",
       "      <td>Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38913458</td>\n",
       "      <td>Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1624567479</td>\n",
       "      <td>Accessories</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pid     category\n",
       "0   730616508  Accessories\n",
       "1  1651651025  Accessories\n",
       "2    62612974  Accessories\n",
       "3    38913458  Accessories\n",
       "4  1624567479  Accessories"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--- Randomly select 100 pids from the 3700 projects ---#\n",
    "\n",
    "# read pjson\n",
    "pjson = read_feather('data/v1/pjson.feather')\n",
    "pjson = pjson[pjson.category.isin(['Accessories', 'Product Design'])]\n",
    "\n",
    "# stats: 2940 Accessories (78%), ~760 Product Design (22%)\n",
    "# pjson.category.value_counts()\n",
    "\n",
    "# stratefied sampling\n",
    "# sample in total 100 images\n",
    "sample_size = 100\n",
    "accessories_size = int(sample_size * 0.78)\n",
    "product_design_size = sample_size - accessories_size\n",
    "\n",
    "# sample 78 images from Accessories\n",
    "pid_accessory = pjson.loc[pjson.category=='Accessories', ['pid', 'category']].sample(accessories_size)\n",
    "\n",
    "# sample 22 images from Product Design\n",
    "pid_design = pjson.loc[pjson.category=='Product Design', ['pid', 'category']].sample(product_design_size)\n",
    "\n",
    "# the final sample\n",
    "sampled_pid = pd.concat([pid_accessory, pid_design], ignore_index=True)\n",
    "sampled_pid.head()\n",
    "\n",
    "# save the sampled pids\n",
    "sampled_pid.to_feather('data/v2/100-sampled-kick-images/sample_pids.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Use `sampled_pid` to select the corresponding 100 images ---#\n",
    "\n",
    "img_root = Path('/home/yu/chaoyang/research-resources/kickstart-raw-from-amrita/kickstarter-image')\n",
    "\n",
    "for pid in sampled_pid.pid.to_list():\n",
    "    # get the image path\n",
    "    img_path = img_root/pid/'profile_full.jpg'\n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "\n",
    "    # create new image name (pid.jpg)\n",
    "    new_img_path = f'data/v2/100-sampled-kick-images/{pid}.jpg'\n",
    "\n",
    "    # copy the image to a new directory\n",
    "    shutil.copy(img_path, new_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pandas dataframe as excel\n",
    "sampled_pid.to_excel('data/v2/100-sampled-kick-images/sample_pids.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
