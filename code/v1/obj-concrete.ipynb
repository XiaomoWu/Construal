{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init\n",
    "\n",
    "This file contains code to compute the image concreteness. The concreteness has two versions:\n",
    "\n",
    "- Entropy version\n",
    "    - Entropy I: Compute the entropy of label_id distribution.\n",
    "    - Entropy II: \n",
    "    \n",
    "- MNI version (based on the shared paper)\n",
    "\n",
    "- Distance-based\n",
    "\n",
    "- Dictionary-based (sum up the concreteness score of all object names in the image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init\n",
    "\n",
    "This file contains code to compute the image concreteness. The concreteness has two versions:\n",
    "\n",
    "- Entropy version\n",
    "    - Entropy I: Compute the entropy of label_id distribution.\n",
    "    - Entropy II: \n",
    "    \n",
    "- MNI version (based on the shared paper)\n",
    "\n",
    "- Distance-based\n",
    "\n",
    "- Dictionary-based (sum up the concreteness score of all object names in the image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNI concreteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import pyarrow.feather as feather\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnext101_32x8d\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, ToPILImage, CenterCrop, RandomResizedCrop, RandomHorizontalFlip\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "wdir = '/home/yu/OneDrive/Construal'\n",
    "IMG_DIR = f'{wdir}/data/kickstarter-data'\n",
    "MODEL_DIR = f'{wdir}/pretrained_models/mmdetection'\n",
    "\n",
    "os.chdir(wdir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get representation\n",
    "\n",
    "For every project image, use a backbone to generate a representation for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Get the `pid` of all projects of all projects\n",
    "pjson = feather.read_feather(f'{WORK_DIR}/data/pjson.feather')\n",
    "pjson = dt.Frame(pjson)\n",
    "pids = pjson[(f.category=='Product Design') | (f.category=='Accessories'), f.pid].to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dl)=118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64c51d2e73942e9b4094c15fa83c92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yu/Software/Anaconda/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "class ReprDataset(Dataset):\n",
    "    def __init__(self, pids):\n",
    "        # check all pids exists\n",
    "        pids = [pid for pid in pids\n",
    "                if os.path.exists(f'{IMG_DIR}/{pid}/profile_full.jpg')]\n",
    "        self.pids = pids\n",
    "        self.transform = Compose([Resize(256),\n",
    "                                  CenterCrop(224),\n",
    "                                  ToTensor(),\n",
    "                                  Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                            std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid = self.pids[idx]\n",
    "        img_path = f'{IMG_DIR}/{pid}/profile_full.jpg'\n",
    "        with PIL.Image.open(img_path) as img:\n",
    "            img = img.convert('RGB')\n",
    "            img = self.transform(img)\n",
    "        return pid, img\n",
    "\n",
    "# load and freeze model     \n",
    "model = resnext101_32x8d(pretrained=True)   \n",
    "model.fc = nn.Identity()\n",
    "\n",
    "device = 'cuda:1'\n",
    "model.to(device)\n",
    "\n",
    "# make dataset/dataloader\n",
    "ds = ReprDataset(pids)\n",
    "dl = DataLoader(ds, shuffle=False, batch_size=32, drop_last=False)\n",
    "print(f'{len(dl)=}')\n",
    "results = {}\n",
    "\n",
    "# run!\n",
    "with torch.no_grad():\n",
    "    for i, (pid, img) in enumerate(tqdm(dl)):\n",
    "        img = img.to(device)\n",
    "        img_repr = model(img)\n",
    "\n",
    "        for p, r in zip(pid, img_repr):\n",
    "            results[p] = r\n",
    "\n",
    "torch.save(results, f'{wdir}/data/obj_repr.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get label-level MNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in ld(\"df_objdet\", force = T): unused argument (force = T)\n",
     "output_type": "error",
     "traceback": [
      "Error in ld(\"df_objdet\", force = T): unused argument (force = T)\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# make text description from label name (R)\n",
    "setwd('/home/yu/OneDrive/Construal')\n",
    "ld('df_objdet', force=T)\n",
    "ld('lvis_dist', force=T)\n",
    "\n",
    "label_id_name_link = lvis_dist[, .(label_id=id, label_name=name)] %>% unique()\n",
    "\n",
    "project_label_names = df_objdet[label_id_name_link, on=.(label_id)\n",
    "      ][prob>=0.5, \n",
    "        .(label_name, label_id),\n",
    "        keyby=.(pid)] %>% unique()\n",
    "\n",
    "sv(project_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load image reprs into an annoy tree\n",
    "reprs = torch.load(f'{wdir}/data/obj_repr.pt')\n",
    "valid_pids = list(reprs.keys())\n",
    "\n",
    "# load image reprs into an annoy tree\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "t = AnnoyIndex(2048, 'angular')  # Length of item vector that will be indexed\n",
    "\n",
    "pid2id = {}\n",
    "for i, (pid, vec) in enumerate(reprs.items()): \n",
    "    # create a map from label_id to an int\n",
    "    pid2id[pid] = i\n",
    "\n",
    "    # add to annoy tree\n",
    "    vec = vec.cpu().numpy()\n",
    "    t.add_item(i, vec)\n",
    "    \n",
    "t.build(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"project_label_names.feather\" (231.7 KB) loaded as \"text\" (<1s) (2021-10-18 3:41 PM)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 561/561 [01:04<00:00,  8.67it/s]\n",
      "100%|██████████| 561/561 [01:11<00:00,  7.89it/s]\n",
      "100%|██████████| 561/561 [01:18<00:00,  7.11it/s]\n",
      "100%|██████████| 561/561 [01:25<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"dt_k10\" saved as \"dt_k10.feather\" (9.5 KB) (<1s) (2021-10-18 3:47 PM)\n",
      "\"dt_k25\" saved as \"dt_k25.feather\" (9.7 KB) (<1s) (2021-10-18 3:47 PM)\n",
      "\"dt_k50\" saved as \"dt_k50.feather\" (9.8 KB) (<1s) (2021-10-18 3:47 PM)\n",
      "\"dt_k100\" saved as \"dt_k100.feather\" (10.0 KB) (<1s) (2021-10-18 3:47 PM)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# build a set for unique objects\n",
    "ld('project_label_names', 'text')\n",
    "text = text[[f.pid == p for p in valid_pids],:]\n",
    "W = dt.unique(text['label_name']).to_list()[0]\n",
    "\n",
    "# set k\n",
    "k = 25\n",
    "\n",
    "def make_mni(k, t):\n",
    "    mni_dict = {}\n",
    "\n",
    "    for w in tqdm(W):\n",
    "        Vw = text[f.label_name==w, [f.pid]]\n",
    "        Vw = dt.unique(Vw).to_list()[0]\n",
    "        Vw = [pid2id[pid] for pid in Vw]\n",
    "        Vw = set(Vw)\n",
    "        \n",
    "        a = 0\n",
    "        for v in Vw:\n",
    "            NN_v = set(t.get_nns_by_item(v, k)) - set([v])\n",
    "            a += len(Vw.intersection(NN_v))\n",
    "\n",
    "        mni = a/len(Vw)\n",
    "        adj_mni = mni/(len(Vw)*k)*819\n",
    "\n",
    "        mni_dict[w] = adj_mni\n",
    "\n",
    "    frame = dt.Frame(label=list(mni_dict.keys()), mni=list(mni_dict.values()))[:,:,dt.sort(-f.mni)]\n",
    "\n",
    "    return frame\n",
    "\n",
    "dt_k10 = make_mni(10, t)\n",
    "dt_k25 = make_mni(25, t)\n",
    "dt_k50 = make_mni(50, t)\n",
    "dt_k100 = make_mni(100, t)\n",
    "\n",
    "sv('dt_k10')\n",
    "sv('dt_k25')\n",
    "sv('dt_k50')\n",
    "sv('dt_k100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in ld(pid_weighted_mni_entropy, path = \"/home/yu/OneDrive/Construal/data/sharing\"): Cannot find \"pid_weighted_mni_entropy\" with possible extensions (\"rds\", \"feather\")\n",
     "output_type": "error",
     "traceback": [
      "Error in ld(pid_weighted_mni_entropy, path = \"/home/yu/OneDrive/Construal/data/sharing\"): Cannot find \"pid_weighted_mni_entropy\" with possible extensions (\"rds\", \"feather\")\nTraceback:\n",
      "1. ld(pid_weighted_mni_entropy, path = \"/home/yu/OneDrive/Construal/data/sharing\")",
      "2. stop(sprintf(\"Cannot find \\\"%s\\\" with possible extensions (\\\"rds\\\", \\\"feather\\\")\", \n .     filename))"
     ]
    }
   ],
   "source": [
    "ld(pid_weighted_mni_entropy, path='/home/yu/OneDrive/Construal/data/sharing')\n",
    "pid_weighted_mni_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"dt_k10.feather\" (9.5 KB) loaded (0.02 secs) (2021-10-18 3:48 PM)\n",
      "\"dt_k25.feather\" (9.7 KB) loaded (0 secs) (2021-10-18 3:48 PM)\n",
      "\"dt_k50.feather\" (9.8 KB) loaded (0 secs) (2021-10-18 3:48 PM)\n",
      "\"dt_k100.feather\" (10 KB) loaded (0 secs) (2021-10-18 3:48 PM)\n",
      "\"mni\" saved as \"mni.feather\" (16.6 KB) (0.01 secs, 2021-10-18 15:48:31)\n"
     ]
    }
   ],
   "source": [
    "# R\n",
    "\n",
    "setwd('/home/yu/OneDrive/Construal')\n",
    "ld(dt_k10)\n",
    "ld(dt_k25)\n",
    "ld(dt_k50)\n",
    "ld(dt_k100)\n",
    "\n",
    "setnames(dt_k10, 'mni', 'mni_k10')\n",
    "setnames(dt_k25, 'mni', 'mni_k25')\n",
    "setnames(dt_k50, 'mni', 'mni_k50')\n",
    "setnames(dt_k100, 'mni', 'mni_k100')\n",
    "\n",
    "mni = dt_k10[dt_k25, on='label'\n",
    "    ][dt_k50, on='label'\n",
    "    ][dt_k100, on='label']\n",
    "sv(mni)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get pid-level MNI\n",
    "\n",
    "> Notes:\n",
    "> - Only select the \"project_full.jpg\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prob is NOT normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"df_objdet.feather\" (82.1 MB) loaded (0.68 secs) (2021-10-18 3:49 PM)\n",
      "\"mni.feather\" (16.6 KB) loaded (0 secs) (2021-10-18 3:49 PM)\n",
      "\"lvis_dist.feather\" (80.4 KB) loaded (0 secs) (2021-10-18 3:49 PM)\n",
      "\"pid_mni_weighted\" saved as \"pid_mni_weighted.feather\" (280.5 KB) (0 secs, 2021-10-18 15:49:08)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.table: 6 × 9</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>pid</th><th scope=col>mni_k10_weighted_unnormalized</th><th scope=col>mni_k10_weighted_normalized</th><th scope=col>mni_k25_weighted_unnormalized</th><th scope=col>mni_k25_weighted_normalized</th><th scope=col>mni_k50_weighted_unnormalized</th><th scope=col>mni_k50_weighted_normalized</th><th scope=col>mni_k100_weighted_unnormalized</th><th scope=col>mni_k100_weighted_normalized</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1000117510</td><td>6.813139</td><td>0.5636056</td><td>6.839113</td><td>0.5657542</td><td>6.583594</td><td>0.5446169</td><td>5.5777506</td><td>0.4614101</td></tr>\n",
       "\t<tr><td>1000234595</td><td>3.579877</td><td>0.7318273</td><td>3.249338</td><td>0.6642557</td><td>2.681237</td><td>0.5481200</td><td>2.3289726</td><td>0.4761073</td></tr>\n",
       "\t<tr><td>1000426032</td><td>3.050856</td><td>0.6679252</td><td>3.516629</td><td>0.7698972</td><td>3.024262</td><td>0.6621031</td><td>2.6430163</td><td>0.5786367</td></tr>\n",
       "\t<tr><td>1001190550</td><td>4.963197</td><td>1.0224062</td><td>4.201131</td><td>0.8654224</td><td>3.762350</td><td>0.7750346</td><td>3.2009327</td><td>0.6593841</td></tr>\n",
       "\t<tr><td>1001259618</td><td>1.297990</td><td>0.6367848</td><td>1.012390</td><td>0.4966715</td><td>1.152132</td><td>0.5652277</td><td>0.9731432</td><td>0.4774172</td></tr>\n",
       "\t<tr><td>1003704820</td><td>4.909002</td><td>0.6681071</td><td>4.994311</td><td>0.6797174</td><td>4.712035</td><td>0.6413001</td><td>4.1226527</td><td>0.5610862</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.table: 6 × 9\n",
       "\\begin{tabular}{lllllllll}\n",
       " pid & mni\\_k10\\_weighted\\_unnormalized & mni\\_k10\\_weighted\\_normalized & mni\\_k25\\_weighted\\_unnormalized & mni\\_k25\\_weighted\\_normalized & mni\\_k50\\_weighted\\_unnormalized & mni\\_k50\\_weighted\\_normalized & mni\\_k100\\_weighted\\_unnormalized & mni\\_k100\\_weighted\\_normalized\\\\\n",
       " <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1000117510 & 6.813139 & 0.5636056 & 6.839113 & 0.5657542 & 6.583594 & 0.5446169 & 5.5777506 & 0.4614101\\\\\n",
       "\t 1000234595 & 3.579877 & 0.7318273 & 3.249338 & 0.6642557 & 2.681237 & 0.5481200 & 2.3289726 & 0.4761073\\\\\n",
       "\t 1000426032 & 3.050856 & 0.6679252 & 3.516629 & 0.7698972 & 3.024262 & 0.6621031 & 2.6430163 & 0.5786367\\\\\n",
       "\t 1001190550 & 4.963197 & 1.0224062 & 4.201131 & 0.8654224 & 3.762350 & 0.7750346 & 3.2009327 & 0.6593841\\\\\n",
       "\t 1001259618 & 1.297990 & 0.6367848 & 1.012390 & 0.4966715 & 1.152132 & 0.5652277 & 0.9731432 & 0.4774172\\\\\n",
       "\t 1003704820 & 4.909002 & 0.6681071 & 4.994311 & 0.6797174 & 4.712035 & 0.6413001 & 4.1226527 & 0.5610862\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.table: 6 × 9\n",
       "\n",
       "| pid &lt;chr&gt; | mni_k10_weighted_unnormalized &lt;dbl&gt; | mni_k10_weighted_normalized &lt;dbl&gt; | mni_k25_weighted_unnormalized &lt;dbl&gt; | mni_k25_weighted_normalized &lt;dbl&gt; | mni_k50_weighted_unnormalized &lt;dbl&gt; | mni_k50_weighted_normalized &lt;dbl&gt; | mni_k100_weighted_unnormalized &lt;dbl&gt; | mni_k100_weighted_normalized &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| 1000117510 | 6.813139 | 0.5636056 | 6.839113 | 0.5657542 | 6.583594 | 0.5446169 | 5.5777506 | 0.4614101 |\n",
       "| 1000234595 | 3.579877 | 0.7318273 | 3.249338 | 0.6642557 | 2.681237 | 0.5481200 | 2.3289726 | 0.4761073 |\n",
       "| 1000426032 | 3.050856 | 0.6679252 | 3.516629 | 0.7698972 | 3.024262 | 0.6621031 | 2.6430163 | 0.5786367 |\n",
       "| 1001190550 | 4.963197 | 1.0224062 | 4.201131 | 0.8654224 | 3.762350 | 0.7750346 | 3.2009327 | 0.6593841 |\n",
       "| 1001259618 | 1.297990 | 0.6367848 | 1.012390 | 0.4966715 | 1.152132 | 0.5652277 | 0.9731432 | 0.4774172 |\n",
       "| 1003704820 | 4.909002 | 0.6681071 | 4.994311 | 0.6797174 | 4.712035 | 0.6413001 | 4.1226527 | 0.5610862 |\n",
       "\n"
      ],
      "text/plain": [
       "  pid        mni_k10_weighted_unnormalized mni_k10_weighted_normalized\n",
       "1 1000117510 6.813139                      0.5636056                  \n",
       "2 1000234595 3.579877                      0.7318273                  \n",
       "3 1000426032 3.050856                      0.6679252                  \n",
       "4 1001190550 4.963197                      1.0224062                  \n",
       "5 1001259618 1.297990                      0.6367848                  \n",
       "6 1003704820 4.909002                      0.6681071                  \n",
       "  mni_k25_weighted_unnormalized mni_k25_weighted_normalized\n",
       "1 6.839113                      0.5657542                  \n",
       "2 3.249338                      0.6642557                  \n",
       "3 3.516629                      0.7698972                  \n",
       "4 4.201131                      0.8654224                  \n",
       "5 1.012390                      0.4966715                  \n",
       "6 4.994311                      0.6797174                  \n",
       "  mni_k50_weighted_unnormalized mni_k50_weighted_normalized\n",
       "1 6.583594                      0.5446169                  \n",
       "2 2.681237                      0.5481200                  \n",
       "3 3.024262                      0.6621031                  \n",
       "4 3.762350                      0.7750346                  \n",
       "5 1.152132                      0.5652277                  \n",
       "6 4.712035                      0.6413001                  \n",
       "  mni_k100_weighted_unnormalized mni_k100_weighted_normalized\n",
       "1 5.5777506                      0.4614101                   \n",
       "2 2.3289726                      0.4761073                   \n",
       "3 2.6430163                      0.5786367                   \n",
       "4 3.2009327                      0.6593841                   \n",
       "5 0.9731432                      0.4774172                   \n",
       "6 4.1226527                      0.5610862                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wdir = '/home/yu/OneDrive/Construal'\n",
    "setwd(wdir)\n",
    "\n",
    "ld(df_objdet, force=T)\n",
    "ld(mni, force=T)\n",
    "ld(lvis_dist)\n",
    "\n",
    "label_id_name_link = lvis_dist[, .(label_id=id, label_name=name)] %>% unique()\n",
    "\n",
    "pid_mni_weighted = df_objdet[label_id_name_link, on=.(label_id)\n",
    "    ][jpg=='profile_full.jpg', .(pid, label_name, inst_id, prob)\n",
    "    ][mni, on=c('label_name==label'), nomatch=NULL\n",
    "    ][, .(mni_k10_weighted_unnormalized=sum(prob*mni_k10),\n",
    "          mni_k10_weighted_normalized=sum(prob*mni_k10)/sum(prob),\n",
    "          mni_k25_weighted_unnormalized=sum(prob*mni_k25),\n",
    "          mni_k25_weighted_normalized=sum(prob*mni_k25)/sum(prob),\n",
    "          mni_k50_weighted_unnormalized=sum(prob*mni_k50),\n",
    "          mni_k50_weighted_normalized=sum(prob*mni_k50)/sum(prob),\n",
    "          mni_k100_weighted_unnormalized=sum(prob*mni_k100),\n",
    "          mni_k100_weighted_normalized=sum(prob*mni_k100)/sum(prob)),\n",
    "      keyby=.(pid)\n",
    "    ][order(pid)]\n",
    "\n",
    "sv(pid_mni_weighted)\n",
    "pid_mni_weighted %>% head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "\n",
    "Image concreteness based entropy. The steps are:\n",
    "- Compute the entropy for every label_id\n",
    "- For every PID, aggregate the label-level entropy of all the containing images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get label-level entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_objdet (82.1 MB) already loaded, will NOT load again! (0 secs) (2021-10-18 3:49 PM)\n",
      "\"entropy\" saved as \"entropy.feather\" (15.3 KB) (0 secs, 2021-10-18 15:49:53)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.table: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>label_id</th><th scope=col>entropy</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>4.239553</td></tr>\n",
       "\t<tr><td>2</td><td>5.161556</td></tr>\n",
       "\t<tr><td>3</td><td>5.324020</td></tr>\n",
       "\t<tr><td>4</td><td>3.382207</td></tr>\n",
       "\t<tr><td>5</td><td>6.059459</td></tr>\n",
       "\t<tr><td>6</td><td>1.802409</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.table: 6 × 2\n",
       "\\begin{tabular}{ll}\n",
       " label\\_id & entropy\\\\\n",
       " <int> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1 & 4.239553\\\\\n",
       "\t 2 & 5.161556\\\\\n",
       "\t 3 & 5.324020\\\\\n",
       "\t 4 & 3.382207\\\\\n",
       "\t 5 & 6.059459\\\\\n",
       "\t 6 & 1.802409\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.table: 6 × 2\n",
       "\n",
       "| label_id &lt;int&gt; | entropy &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| 1 | 4.239553 |\n",
       "| 2 | 5.161556 |\n",
       "| 3 | 5.324020 |\n",
       "| 4 | 3.382207 |\n",
       "| 5 | 6.059459 |\n",
       "| 6 | 1.802409 |\n",
       "\n"
      ],
      "text/plain": [
       "  label_id entropy \n",
       "1 1        4.239553\n",
       "2 2        5.161556\n",
       "3 3        5.324020\n",
       "4 4        3.382207\n",
       "5 5        6.059459\n",
       "6 6        1.802409"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(arrow)\n",
    "library(stringr)\n",
    "\n",
    "wdir = '/home/yu/OneDrive/Construal/'\n",
    "setwd(wdir)\n",
    "\n",
    "# Substitute with your data path\n",
    "ld(df_objdet)\n",
    "\n",
    "# compute probabilities of labels\n",
    "obj = df_objdet[jpg=='profile_full.jpg'\n",
    "    ][, .(prob=sum(prob)), keyby=.(label_id, pid)\n",
    "    ][, .(prob=prob/sum(prob), id=1:.N), keyby=.(label_id)]\n",
    "\n",
    "# extroploate so that every label has 819 obs\n",
    "CJ = data.table(label_id=rep(1:1202, each=819), id=rep(1:819, 1202))\n",
    "obj = obj[CJ, on=c('label_id', 'id')\n",
    "    ][is.na(prob), ':='(prob=0)\n",
    "    ][, ':='(prob=prob+1e-5)\n",
    "    ][, ':='(prob=prob/sum(prob)), keyby=.(label_id)]\n",
    "\n",
    "# compute entropy\n",
    "entropy = obj[, .(entropy=-sum(prob*log(prob))), keyby=.(label_id)]\n",
    "sv(entropy)\n",
    "entropy %>% head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get pid-level entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"df_objdet.feather\" (82.1 MB) loaded (0.52 secs) (2021-10-18 3:50 PM)\n",
      "\"entropy.feather\" (15.3 KB) loaded (0 secs) (2021-10-18 3:50 PM)\n",
      "lvis_dist (80.4 KB) already loaded, will NOT load again! (0 secs) (2021-10-18 3:50 PM)\n",
      "\"pid_entropy_weighted\" saved as \"pid_entropy_weighted.feather\" (102.6 KB) (0 secs, 2021-10-18 15:50:09)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.table: 6 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>pid</th><th scope=col>entropy_weighted_unnormalized</th><th scope=col>entropy_weighted_normalized</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1000117510</td><td>71.16351</td><td>5.375395</td></tr>\n",
       "\t<tr><td>1000234595</td><td>29.46907</td><td>5.201943</td></tr>\n",
       "\t<tr><td>1000426032</td><td>24.25037</td><td>5.239405</td></tr>\n",
       "\t<tr><td>1001190550</td><td>25.03649</td><td>4.954908</td></tr>\n",
       "\t<tr><td>1001259618</td><td>10.87286</td><td>5.065028</td></tr>\n",
       "\t<tr><td>1003704820</td><td>39.81941</td><td>5.240943</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.table: 6 × 3\n",
       "\\begin{tabular}{lll}\n",
       " pid & entropy\\_weighted\\_unnormalized & entropy\\_weighted\\_normalized\\\\\n",
       " <chr> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1000117510 & 71.16351 & 5.375395\\\\\n",
       "\t 1000234595 & 29.46907 & 5.201943\\\\\n",
       "\t 1000426032 & 24.25037 & 5.239405\\\\\n",
       "\t 1001190550 & 25.03649 & 4.954908\\\\\n",
       "\t 1001259618 & 10.87286 & 5.065028\\\\\n",
       "\t 1003704820 & 39.81941 & 5.240943\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.table: 6 × 3\n",
       "\n",
       "| pid &lt;chr&gt; | entropy_weighted_unnormalized &lt;dbl&gt; | entropy_weighted_normalized &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1000117510 | 71.16351 | 5.375395 |\n",
       "| 1000234595 | 29.46907 | 5.201943 |\n",
       "| 1000426032 | 24.25037 | 5.239405 |\n",
       "| 1001190550 | 25.03649 | 4.954908 |\n",
       "| 1001259618 | 10.87286 | 5.065028 |\n",
       "| 1003704820 | 39.81941 | 5.240943 |\n",
       "\n"
      ],
      "text/plain": [
       "  pid        entropy_weighted_unnormalized entropy_weighted_normalized\n",
       "1 1000117510 71.16351                      5.375395                   \n",
       "2 1000234595 29.46907                      5.201943                   \n",
       "3 1000426032 24.25037                      5.239405                   \n",
       "4 1001190550 25.03649                      4.954908                   \n",
       "5 1001259618 10.87286                      5.065028                   \n",
       "6 1003704820 39.81941                      5.240943                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ld(df_objdet, force=T)\n",
    "ld(entropy, force=T)\n",
    "ld(lvis_dist)\n",
    "\n",
    "label_id_name_link = lvis_dist[, .(label_id=id, label_name=name)] %>% unique()\n",
    "\n",
    "pid_entropy_weighted = df_objdet[label_id_name_link, on=.(label_id)\n",
    "    ][jpg=='profile_full.jpg', .(pid, label_id, inst_id, prob)\n",
    "    ][entropy, on=c('label_id'), nomatch=NULL\n",
    "    ][, .(entropy_weighted_unnormalized=sum(prob*entropy),\n",
    "          entropy_weighted_normalized=sum(prob*entropy)/sum(prob)),\n",
    "      keyby=.(pid)\n",
    "    ][order(pid)]\n",
    "\n",
    "sv(pid_entropy_weighted)\n",
    "\n",
    "pid_entropy_weighted %>% head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid_entropy_weighted (102.6 KB) already loaded, will NOT load again! (0 secs) (2021-10-18 3:50 PM)\n",
      "pid_mni_weighted (280.5 KB) already loaded, will NOT load again! (0 secs) (2021-10-18 3:50 PM)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.table: 6 × 11</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>pid</th><th scope=col>entropy_weighted_unnormalized</th><th scope=col>entropy_weighted_normalized</th><th scope=col>mni_k10_weighted_unnormalized</th><th scope=col>mni_k10_weighted_normalized</th><th scope=col>mni_k25_weighted_unnormalized</th><th scope=col>mni_k25_weighted_normalized</th><th scope=col>mni_k50_weighted_unnormalized</th><th scope=col>mni_k50_weighted_normalized</th><th scope=col>mni_k100_weighted_unnormalized</th><th scope=col>mni_k100_weighted_normalized</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1000117510</td><td>71.16351</td><td>5.375395</td><td>6.813139</td><td>0.5636056</td><td>6.839113</td><td>0.5657542</td><td>6.583594</td><td>0.5446169</td><td>5.5777506</td><td>0.4614101</td></tr>\n",
       "\t<tr><td>1000234595</td><td>29.46907</td><td>5.201943</td><td>3.579877</td><td>0.7318273</td><td>3.249338</td><td>0.6642557</td><td>2.681237</td><td>0.5481200</td><td>2.3289726</td><td>0.4761073</td></tr>\n",
       "\t<tr><td>1000426032</td><td>24.25037</td><td>5.239405</td><td>3.050856</td><td>0.6679252</td><td>3.516629</td><td>0.7698972</td><td>3.024262</td><td>0.6621031</td><td>2.6430163</td><td>0.5786367</td></tr>\n",
       "\t<tr><td>1001190550</td><td>25.03649</td><td>4.954908</td><td>4.963197</td><td>1.0224062</td><td>4.201131</td><td>0.8654224</td><td>3.762350</td><td>0.7750346</td><td>3.2009327</td><td>0.6593841</td></tr>\n",
       "\t<tr><td>1001259618</td><td>10.87286</td><td>5.065028</td><td>1.297990</td><td>0.6367848</td><td>1.012390</td><td>0.4966715</td><td>1.152132</td><td>0.5652277</td><td>0.9731432</td><td>0.4774172</td></tr>\n",
       "\t<tr><td>1003704820</td><td>39.81941</td><td>5.240943</td><td>4.909002</td><td>0.6681071</td><td>4.994311</td><td>0.6797174</td><td>4.712035</td><td>0.6413001</td><td>4.1226527</td><td>0.5610862</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.table: 6 × 11\n",
       "\\begin{tabular}{lllllllllll}\n",
       " pid & entropy\\_weighted\\_unnormalized & entropy\\_weighted\\_normalized & mni\\_k10\\_weighted\\_unnormalized & mni\\_k10\\_weighted\\_normalized & mni\\_k25\\_weighted\\_unnormalized & mni\\_k25\\_weighted\\_normalized & mni\\_k50\\_weighted\\_unnormalized & mni\\_k50\\_weighted\\_normalized & mni\\_k100\\_weighted\\_unnormalized & mni\\_k100\\_weighted\\_normalized\\\\\n",
       " <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1000117510 & 71.16351 & 5.375395 & 6.813139 & 0.5636056 & 6.839113 & 0.5657542 & 6.583594 & 0.5446169 & 5.5777506 & 0.4614101\\\\\n",
       "\t 1000234595 & 29.46907 & 5.201943 & 3.579877 & 0.7318273 & 3.249338 & 0.6642557 & 2.681237 & 0.5481200 & 2.3289726 & 0.4761073\\\\\n",
       "\t 1000426032 & 24.25037 & 5.239405 & 3.050856 & 0.6679252 & 3.516629 & 0.7698972 & 3.024262 & 0.6621031 & 2.6430163 & 0.5786367\\\\\n",
       "\t 1001190550 & 25.03649 & 4.954908 & 4.963197 & 1.0224062 & 4.201131 & 0.8654224 & 3.762350 & 0.7750346 & 3.2009327 & 0.6593841\\\\\n",
       "\t 1001259618 & 10.87286 & 5.065028 & 1.297990 & 0.6367848 & 1.012390 & 0.4966715 & 1.152132 & 0.5652277 & 0.9731432 & 0.4774172\\\\\n",
       "\t 1003704820 & 39.81941 & 5.240943 & 4.909002 & 0.6681071 & 4.994311 & 0.6797174 & 4.712035 & 0.6413001 & 4.1226527 & 0.5610862\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.table: 6 × 11\n",
       "\n",
       "| pid &lt;chr&gt; | entropy_weighted_unnormalized &lt;dbl&gt; | entropy_weighted_normalized &lt;dbl&gt; | mni_k10_weighted_unnormalized &lt;dbl&gt; | mni_k10_weighted_normalized &lt;dbl&gt; | mni_k25_weighted_unnormalized &lt;dbl&gt; | mni_k25_weighted_normalized &lt;dbl&gt; | mni_k50_weighted_unnormalized &lt;dbl&gt; | mni_k50_weighted_normalized &lt;dbl&gt; | mni_k100_weighted_unnormalized &lt;dbl&gt; | mni_k100_weighted_normalized &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1000117510 | 71.16351 | 5.375395 | 6.813139 | 0.5636056 | 6.839113 | 0.5657542 | 6.583594 | 0.5446169 | 5.5777506 | 0.4614101 |\n",
       "| 1000234595 | 29.46907 | 5.201943 | 3.579877 | 0.7318273 | 3.249338 | 0.6642557 | 2.681237 | 0.5481200 | 2.3289726 | 0.4761073 |\n",
       "| 1000426032 | 24.25037 | 5.239405 | 3.050856 | 0.6679252 | 3.516629 | 0.7698972 | 3.024262 | 0.6621031 | 2.6430163 | 0.5786367 |\n",
       "| 1001190550 | 25.03649 | 4.954908 | 4.963197 | 1.0224062 | 4.201131 | 0.8654224 | 3.762350 | 0.7750346 | 3.2009327 | 0.6593841 |\n",
       "| 1001259618 | 10.87286 | 5.065028 | 1.297990 | 0.6367848 | 1.012390 | 0.4966715 | 1.152132 | 0.5652277 | 0.9731432 | 0.4774172 |\n",
       "| 1003704820 | 39.81941 | 5.240943 | 4.909002 | 0.6681071 | 4.994311 | 0.6797174 | 4.712035 | 0.6413001 | 4.1226527 | 0.5610862 |\n",
       "\n"
      ],
      "text/plain": [
       "  pid        entropy_weighted_unnormalized entropy_weighted_normalized\n",
       "1 1000117510 71.16351                      5.375395                   \n",
       "2 1000234595 29.46907                      5.201943                   \n",
       "3 1000426032 24.25037                      5.239405                   \n",
       "4 1001190550 25.03649                      4.954908                   \n",
       "5 1001259618 10.87286                      5.065028                   \n",
       "6 1003704820 39.81941                      5.240943                   \n",
       "  mni_k10_weighted_unnormalized mni_k10_weighted_normalized\n",
       "1 6.813139                      0.5636056                  \n",
       "2 3.579877                      0.7318273                  \n",
       "3 3.050856                      0.6679252                  \n",
       "4 4.963197                      1.0224062                  \n",
       "5 1.297990                      0.6367848                  \n",
       "6 4.909002                      0.6681071                  \n",
       "  mni_k25_weighted_unnormalized mni_k25_weighted_normalized\n",
       "1 6.839113                      0.5657542                  \n",
       "2 3.249338                      0.6642557                  \n",
       "3 3.516629                      0.7698972                  \n",
       "4 4.201131                      0.8654224                  \n",
       "5 1.012390                      0.4966715                  \n",
       "6 4.994311                      0.6797174                  \n",
       "  mni_k50_weighted_unnormalized mni_k50_weighted_normalized\n",
       "1 6.583594                      0.5446169                  \n",
       "2 2.681237                      0.5481200                  \n",
       "3 3.024262                      0.6621031                  \n",
       "4 3.762350                      0.7750346                  \n",
       "5 1.152132                      0.5652277                  \n",
       "6 4.712035                      0.6413001                  \n",
       "  mni_k100_weighted_unnormalized mni_k100_weighted_normalized\n",
       "1 5.5777506                      0.4614101                   \n",
       "2 2.3289726                      0.4761073                   \n",
       "3 2.6430163                      0.5786367                   \n",
       "4 3.2009327                      0.6593841                   \n",
       "5 0.9731432                      0.4774172                   \n",
       "6 4.1226527                      0.5610862                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"pid_weighted_mni_entropy\" saved as \"pid_weighted_mni_entropy.feather\" (339.8 KB) (0 secs, 2021-10-18 15:50:17)\n"
     ]
    }
   ],
   "source": [
    "# Combine both entropy and MNI into one dataset\n",
    "ld(pid_entropy_weighted)\n",
    "ld(pid_mni_weighted)\n",
    "\n",
    "pid_weighted_mni_entropy = pid_entropy_weighted[pid_mni_weighted, on=.(pid), nomatch=NULL]\n",
    "pid_weighted_mni_entropy %>% head()\n",
    "sv(pid_weighted_mni_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid_weighted_mni_entropy (339.8 KB) already loaded, will NOT load again! (0 secs) (2021-10-18 3:50 PM)\n"
     ]
    }
   ],
   "source": [
    "wdir = '/home/yu/OneDrive/Construal'\n",
    "setwd(wdir)\n",
    "ld(pid_weighted_mni_entropy)\n",
    "fwrite(pid_weighted_mni_entropy, 'data/sharing/pid_weighted_mni_entropy.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-text Interaction\n",
    "\n",
    "Distance-baesd concreteness. Steps are:\n",
    "- For every PID, Tokenize the image headline. \n",
    "- Compute the embedding of every token in the headline. These embeddings form a cluster.\n",
    "- For every label_name of the PID, compute its embedding. Again, the label_names form another cluster.\n",
    "- Compute the distance:\n",
    "    - Method I: compute the average distance between the clusters\n",
    "    - Method II: compute the shorted distance between the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "\n",
    "from datatable import f\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances, manhattan_distances\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from tqdm.auto import tqdm\n",
    "from utilpy import sv, ld\n",
    "\n",
    "wdir = '/home/yu/OneDrive/Construal'\n",
    "IMG_DIR = f'{wdir}/data/kickstarter-data'\n",
    "MODEL_DIR = f'{wdir}/pretrained_models/mmdetection'\n",
    "\n",
    "os.chdir(wdir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get emb from title text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize the image titles\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "glove = torchtext.vocab.GloVe()\n",
    "fasttext = torchtext.vocab.FastText()\n",
    "charngram = torchtext.vocab.CharNGram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ------ covert text to tokens --------------\n",
    "pjson = ld('pjson')\n",
    "\n",
    "titles = pjson[\n",
    "    (f.category=='Product Design') | (f.category=='Accessories'), \n",
    "    f.title].to_list()[0]\n",
    "\n",
    "descs = pjson[\n",
    "    (f.category=='Product Design') | (f.category=='Accessories'), \n",
    "    f.project_desc].to_list()[0]\n",
    "    \n",
    "pids = pjson[\n",
    "    (f.category=='Product Design') | (f.category=='Accessories'), \n",
    "    f.pid].to_list()[0]\n",
    "\n",
    "# get the tokenized title\n",
    "title_tokens = []\n",
    "for i, title in enumerate(nlp.pipe(titles)):\n",
    "    tokens = [t.text for t in title\n",
    "              if t.pos_ in ['NOUN', 'PROPN', 'PRON']]\n",
    "\n",
    "    # if there's no NOUN tokens in the title, we include them all\n",
    "    if len(tokens) == 0:\n",
    "        tokens = [t.text for t in title]\n",
    "    title_tokens.append(tokens)\n",
    "\n",
    "# get the tokenized proj_desc\n",
    "desc_tokens = []\n",
    "for i, desc in enumerate(nlp.pipe(descs)):\n",
    "    tokens = [t.text for t in desc[:200]\n",
    "              if t.pos_ in ['NOUN', 'PROPN', 'PRON']]\n",
    "\n",
    "    # if there's no NOUN tokens in the title, we include them all\n",
    "    if len(tokens) == 0:\n",
    "        tokens = [t.text for t in title]\n",
    "    desc_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ---------- convert tokens to embs ----------------\n",
    "\n",
    "def get_emb(vocab, tokens):\n",
    "    # get the embedding for every title token\n",
    "    emb_output = {}\n",
    "    for i, (pid, token) in enumerate(zip(pids, tokens)):\n",
    "        emb = []\n",
    "        for t in token:\n",
    "            t = t.lower() if isinstance(vocab, torchtext.vocab.FastText) else t\n",
    "            emb.append(vocab[t].squeeze())\n",
    "        \n",
    "        emb = torch.stack(emb)\n",
    "\n",
    "        emb_output[pid] = emb\n",
    "    return emb_output\n",
    "\n",
    "title_emb_glove = get_emb(glove, title_tokens)\n",
    "title_emb_fasttext = get_emb(fasttext, title_tokens)\n",
    "title_emb_charngram = get_emb(charngram, title_tokens)\n",
    "\n",
    "desc_emb_glove = get_emb(glove, desc_tokens)\n",
    "desc_emb_fasttext = get_emb(fasttext, desc_tokens)\n",
    "desc_emb_charngram = get_emb(charngram, desc_tokens)\n",
    "\n",
    "torch.save(title_emb_glove, 'data/title_emb_glove.pt')\n",
    "torch.save(title_emb_fasttext, 'data/title_emb_fasttext.pt')\n",
    "torch.save(title_emb_charngram, 'data/title_emb_charngram.pt')\n",
    "\n",
    "torch.save(desc_emb_glove, 'data/desc_emb_glove.pt')\n",
    "torch.save(desc_emb_fasttext, 'data/desc_emb_fasttext.pt')\n",
    "torch.save(desc_emb_charngram, 'data/desc_emb_charngram.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get emb from image obj name\n",
    "\n",
    "Here we generate the \"centrioid\" of object name embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"df_objdet.feather\" (112.2 MB) loaded (1 secs) (2021-10-18 4:47 PM)\n",
      "\"objtxt\" saved as \"objtxt.feather\" (9.1 MB) (0.05 secs, 2021-10-18 16:47:20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.table: 6 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>pid</th><th scope=col>prob</th><th scope=col>label_id</th><th scope=col>label_name</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1000117510</td><td>0.006866673</td><td> 5</td><td>alcohol  </td></tr>\n",
       "\t<tr><td>1000117510</td><td>0.005171574</td><td>11</td><td>antenna  </td></tr>\n",
       "\t<tr><td>1000117510</td><td>0.006819463</td><td>27</td><td>avocado  </td></tr>\n",
       "\t<tr><td>1000117510</td><td>0.003172611</td><td>61</td><td>basket   </td></tr>\n",
       "\t<tr><td>1000117510</td><td>0.004577590</td><td>63</td><td>bass_horn</td></tr>\n",
       "\t<tr><td>1000117510</td><td>0.003549974</td><td>75</td><td>beanie   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.table: 6 × 4\n",
       "\\begin{tabular}{llll}\n",
       " pid & prob & label\\_id & label\\_name\\\\\n",
       " <chr> & <dbl> & <int> & <chr>\\\\\n",
       "\\hline\n",
       "\t 1000117510 & 0.006866673 &  5 & alcohol  \\\\\n",
       "\t 1000117510 & 0.005171574 & 11 & antenna  \\\\\n",
       "\t 1000117510 & 0.006819463 & 27 & avocado  \\\\\n",
       "\t 1000117510 & 0.003172611 & 61 & basket   \\\\\n",
       "\t 1000117510 & 0.004577590 & 63 & bass\\_horn\\\\\n",
       "\t 1000117510 & 0.003549974 & 75 & beanie   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.table: 6 × 4\n",
       "\n",
       "| pid &lt;chr&gt; | prob &lt;dbl&gt; | label_id &lt;int&gt; | label_name &lt;chr&gt; |\n",
       "|---|---|---|---|\n",
       "| 1000117510 | 0.006866673 |  5 | alcohol   |\n",
       "| 1000117510 | 0.005171574 | 11 | antenna   |\n",
       "| 1000117510 | 0.006819463 | 27 | avocado   |\n",
       "| 1000117510 | 0.003172611 | 61 | basket    |\n",
       "| 1000117510 | 0.004577590 | 63 | bass_horn |\n",
       "| 1000117510 | 0.003549974 | 75 | beanie    |\n",
       "\n"
      ],
      "text/plain": [
       "  pid        prob        label_id label_name\n",
       "1 1000117510 0.006866673  5       alcohol   \n",
       "2 1000117510 0.005171574 11       antenna   \n",
       "3 1000117510 0.006819463 27       avocado   \n",
       "4 1000117510 0.003172611 61       basket    \n",
       "5 1000117510 0.004577590 63       bass_horn \n",
       "6 1000117510 0.003549974 75       beanie    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the images classes (R)\n",
    "wdir = '/home/yu/OneDrive/Construal'\n",
    "setwd(wdir)\n",
    "\n",
    "ld(df_objdet, force=T)\n",
    "\n",
    "# objtxt: the text of image objects, grouped by pid\n",
    "# - Frist we aggreate prob of same labels within a pid\n",
    "# - Then we normalize prob so that the probs of each pid sum to one\n",
    "objtxt = df_objdet[!is.na(label_name) & !is.na(prob) & jpg=='profile_full.jpg',\n",
    "    ][, .(prob=sum(prob), label_name=label_name[1]), keyby=.(pid, label_id)\n",
    "    ][, ':='(prob=prob/sum(prob)), keyby=.(pid)\n",
    "    ][, .(pid, prob, label_id, label_name)][order(pid, label_id)]\n",
    "sv(objtxt)\n",
    "objtxt %>% head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize the images classes\n",
    "wdir = '/home/yu/OneDrive/Construal'\n",
    "os.chdir(wdir)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "glove = torchtext.vocab.GloVe()\n",
    "fasttext = torchtext.vocab.FastText()\n",
    "charngram = torchtext.vocab.CharNGram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/3749 [00:00<05:02, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"objtxt.feather\" (9.1 MB) loaded (<1s) (2021-10-18 4:48 PM)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3749/3749 [02:26<00:00, 25.54it/s]\n",
      "100%|██████████| 3749/3749 [02:27<00:00, 25.42it/s]\n",
      "100%|██████████| 3749/3749 [03:15<00:00, 19.13it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_objname_emb(vocab):\n",
    "    objname_emb = {}\n",
    "    \n",
    "    for pid in tqdm(set(objtxt['pid'].to_list()[0])):\n",
    "        # get embs from object names\n",
    "        prob_objname, objname = objtxt[f.pid==pid, [f.prob, f.label_name]].to_list()\n",
    "        prob_objname = np.array(prob_objname) # (N,)\n",
    "\n",
    "        embs = []\n",
    "        for tokens in nlp.pipe(objname):\n",
    "            tokens = tokens.text.split('_')\n",
    "            embs_temp = []\n",
    "            for t in tokens:\n",
    "                t = t.lower()\n",
    "                embs_temp.append(vocab[t])\n",
    "            embs_temp = torch.stack(embs_temp).mean(0).squeeze()\n",
    "            embs.append(embs_temp)\n",
    "\n",
    "        objname_emb[pid] = torch.stack(embs).numpy() # (N,300)\n",
    "\n",
    "    return objname_emb\n",
    "\n",
    "ld('objtxt', force=True)\n",
    "\n",
    "objname_emb_glove = get_objname_emb(glove)\n",
    "objname_emb_fasttext = get_objname_emb(fasttext)\n",
    "objname_emb_charngram = get_objname_emb(charngram)\n",
    "\n",
    "torch.save(objname_emb_glove, 'data/objname_emb_glove.pt')\n",
    "torch.save(objname_emb_fasttext, 'data/objname_emb_fasttext.pt')\n",
    "torch.save(objname_emb_charngram, 'data/objname_emb_charngram.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute dist_concreteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# cretae embeddings\n",
    "wdir = '/home/yu/OneDrive/Construal'\n",
    "os.chdir(wdir)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "title_emb_glove = torch.load('data/title_emb_glove.pt')\n",
    "title_emb_fasttext = torch.load('data/title_emb_fasttext.pt')\n",
    "title_emb_charngram = torch.load('data/title_emb_charngram.pt')\n",
    "\n",
    "desc_emb_glove = torch.load('data/desc_emb_glove.pt')\n",
    "desc_emb_fasttext = torch.load('data/desc_emb_fasttext.pt')\n",
    "desc_emb_charngram = torch.load('data/desc_emb_charngram.pt')\n",
    "\n",
    "objname_emb_glove = torch.load('data/objname_emb_glove.pt')\n",
    "objname_emb_fasttext = torch.load('data/objname_emb_fasttext.pt')\n",
    "objname_emb_charngram = torch.load('data/objname_emb_charngram.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"objtxt.feather\" (9.1 MB) loaded (<1s) (2022-01-12 6:23 PM)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323cc14cb7e849f1b621daaa7d47092c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_distance(text_embs, objname_embs, objtxt):\n",
    "    import datatable as dt\n",
    "\n",
    "    valid_pids = set(objtxt['pid'].to_list()[0]).intersection(set(text_embs.keys()))\n",
    "    valid_pids = list(valid_pids)\n",
    "\n",
    "    results = []\n",
    "    for pid in tqdm(valid_pids):\n",
    "        # get embs from text\n",
    "        text_emb = text_embs[pid]\n",
    "        cent_text = text_emb.numpy().mean(0).reshape(1,-1)\n",
    "\n",
    "        # get embs from object names\n",
    "        objname_emb = objname_embs[pid]\n",
    "        objname_prob = np.array(objtxt[f.pid==pid, f.prob].to_list()[0])\n",
    "        cent_objname = np.matmul(objname_prob, objname_emb).reshape(1,-1)\n",
    "\n",
    "        # compute cluster avg distance\n",
    "        linf_dist = DistanceMetric.get_metric('chebyshev')\n",
    "\n",
    "        cluster_cos_dist = cosine_distances(cent_text, cent_objname)[0][0]\n",
    "        cluster_l2_dist = euclidean_distances(cent_text, cent_objname)[0][0]\n",
    "        cluster_l1_dist = manhattan_distances(cent_text, cent_objname)[0][0]\n",
    "        cluster_linf_dist = linf_dist.pairwise(cent_text, cent_objname)[0][0]\n",
    "\n",
    "        # compute shortest pairwise dist\n",
    "        k_pmax = 3\n",
    "        pmax_idx = objname_prob.argsort()[-k_pmax:]\n",
    "        objname_emb = objname_emb[pmax_idx, :]\n",
    "\n",
    "        shortest_cos_dist = cosine_distances(text_emb, objname_emb).min()\n",
    "        shortest_l2_dist = euclidean_distances(text_emb, objname_emb).min()\n",
    "        shortest_l1_dist = manhattan_distances(text_emb, objname_emb).min()\n",
    "        shortest_linf_dist = linf_dist.pairwise(text_emb, objname_emb).min()\n",
    "        \n",
    "        # append to results\n",
    "        results.append((pid, cluster_cos_dist, cluster_l1_dist, cluster_l2_dist, cluster_linf_dist, shortest_cos_dist, shortest_l1_dist, shortest_l2_dist, shortest_linf_dist))\n",
    "\n",
    "    return dt.Frame(results, names=['pid', 'cluster_cos_dist', 'cluster_l1_dist', 'cluster_l2_dist', 'cluster_linf_dist', 'shortest_cos_dist', 'shortest_l1_dist', 'shortest_l2_dist', 'shortest_linf_dist'])\n",
    "\n",
    "    \n",
    "objtxt = ld('objtxt')\n",
    "\n",
    "# dist_glove_title = get_distance(\n",
    "#     title_emb_glove, objname_emb_glove, objtxt)\n",
    "# dist_fasttext_title = get_distance(\n",
    "#     title_emb_fasttext, objname_emb_fasttext, objtxt)\n",
    "# dist_charngram_title = get_distance(\n",
    "#     title_emb_charngram, objname_emb_charngram, objtxt)\n",
    "\n",
    "# dist_glove_title.to_csv('data/sharing/dist_glove_title.csv')\n",
    "# dist_fasttext_title.to_csv('data/sharing/dist_fasttext_title.csv')\n",
    "# dist_charngram_title.to_csv('data/sharing/dist_charngram_title.csv')\n",
    "\n",
    "\n",
    "dist_glove_desc = get_distance(\n",
    "    desc_emb_glove, objname_emb_glove, objtxt)\n",
    "# dist_fasttext_desc = get_distance(\n",
    "#     desc_emb_fasttext, objname_emb_fasttext, objtxt)\n",
    "# dist_charngram_desc = get_distance(\n",
    "#     desc_emb_charngram, objname_emb_charngram, objtxt)\n",
    "\n",
    "# dist_glove_desc.to_csv('data/sharing/dist_glove_desc.csv')\n",
    "# dist_fasttext_desc.to_csv('data/sharing/dist_fasttext_desc.csv')\n",
    "# dist_charngram_desc.to_csv('data/sharing/dist_charngram_desc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary-based (added in Sep 2023)\n",
    "Task 1:\n",
    "\n",
    "- Get the object names in an image\n",
    "- Get the concreteness score for each object name. The score is based on a dictionary.\n",
    "- The concreteness score of an image is defined as the sum of the concreteness scores of all the containing objects.\n",
    "\n",
    "Task 2:\n",
    "- Same as before, get the object names.\n",
    "- Get the \"frequency\" score based on the google dictionary.\n",
    "- The frequency score is the aggregation of the component object frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "suppressMessages({\n",
    "    library(arrow)\n",
    "})\n",
    "\n",
    "wdir = '/home/yu/OneDrive/Construal'\n",
    "setwd(wdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concreteness (Task 1)\n",
    "\n",
    "Notes:\n",
    "- In the beginning, I only keep objects with prob>=0.5 in the `objdet` dataset. However, this will remove about 1000 pids.\n",
    "- Now, I lower the threshold to prob>=0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# get the object names\n",
    "objdet = read_feather('data/df_objdet.feather') %>% setDT()\n",
    "# objdet[, uniqueN(pid)]\n",
    "\n",
    "objdet = objdet[prob>=0.1, .(pid, object=label_name, inst_id, prob)]\n",
    "# objdet[, uniqueN(pid)]\n",
    "\n",
    "# get pjson\n",
    "pjson = read_feather('data/pjson.feather') %>% setDT()\n",
    "\n",
    "# get the dictionary\n",
    "bscore_dict = fread('data/concreteness_score.csv')[, .(word=str_trim(Word), score=Conc.M)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# get the bscore of each object\n",
    "get_bscore <- function(obj) {\n",
    "    # obj: string, a single object name\n",
    "    # dict: data.table, bscore dictionary\n",
    "    # output: numeric, the concreteness score of the object name. 0 if not found.\n",
    "    bscore_dict[word==obj, fcase(length(score)==0, 0, length(score)>0, score)]\n",
    "}\n",
    "\n",
    "bscore = objdet[, \n",
    "    .(pid, inst_id, prob, object=str_extract_all(object, '[A-Za-z]+') %>% lapply(str_trim))\n",
    "    ][, {\n",
    "    # if object name are composite (e.g., brass handhold), we take the mean \n",
    "    score = sapply(object[[1]], get_bscore) %>% mean()\n",
    "    list(pid, object, prob, inst_id, score)\n",
    "    },\n",
    "    keyby=seq_len(nrow(objdet))\n",
    "    ][, ':='(seq_len=NULL)][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.table: 1 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>pid</th><th scope=col>bscore_sum</th><th scope=col>bscore_mean</th><th scope=col>bscore_pw_sum</th><th scope=col>bscore_pw_mean</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1000117510</td><td>155.896</td><td>4.585176</td><td>31.0826</td><td>4.593648</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.table: 1 × 5\n",
       "\\begin{tabular}{lllll}\n",
       " pid & bscore\\_sum & bscore\\_mean & bscore\\_pw\\_sum & bscore\\_pw\\_mean\\\\\n",
       " <chr> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1000117510 & 155.896 & 4.585176 & 31.0826 & 4.593648\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.table: 1 × 5\n",
       "\n",
       "| pid &lt;chr&gt; | bscore_sum &lt;dbl&gt; | bscore_mean &lt;dbl&gt; | bscore_pw_sum &lt;dbl&gt; | bscore_pw_mean &lt;dbl&gt; |\n",
       "|---|---|---|---|---|\n",
       "| 1000117510 | 155.896 | 4.585176 | 31.0826 | 4.593648 |\n",
       "\n"
      ],
      "text/plain": [
       "  pid        bscore_sum bscore_mean bscore_pw_sum bscore_pw_mean\n",
       "1 1000117510 155.896    4.585176    31.0826       4.593648      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute the bscore for each project\n",
    "img_conc_bscore = bscore[,\n",
    "    .(bscore_sum=sum(score),  # sum of all objects' bscore\n",
    "      bscore_mean=mean(score),  # mean of all objects' bscore \n",
    "      bscore_pw_sum=sum(score*prob),  # sum of all objects' bscore weighted by prob\n",
    "      bscore_pw_mean=sum(score*prob)/sum(prob)), # mean of all objects' bscore weighted by prob \n",
    "    keyby=.(pid)\n",
    "    ]\n",
    "\n",
    "img_conc_bscore[1]\n",
    "fwrite(img_conc_bscore, 'data/sharing/img_conc_bscore_dict_based.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug (Task 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "3636"
      ],
      "text/latex": [
       "3636"
      ],
      "text/markdown": [
       "3636"
      ],
      "text/plain": [
       "[1] 3636"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.table: 1 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>total_pids</th><th scope=col>used_pids</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>3687</td><td>3577</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.table: 1 × 2\n",
       "\\begin{tabular}{ll}\n",
       " total\\_pids & used\\_pids\\\\\n",
       " <int> & <int>\\\\\n",
       "\\hline\n",
       "\t 3687 & 3577\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.table: 1 × 2\n",
       "\n",
       "| total_pids &lt;int&gt; | used_pids &lt;int&gt; |\n",
       "|---|---|\n",
       "| 3687 | 3577 |\n",
       "\n"
      ],
      "text/plain": [
       "  total_pids used_pids\n",
       "1 3687       3577     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get used pids\n",
    "used_pids = fread('data/sharing/missing-pids/used-pids.csv')[, V1]  # outcome is a vector\n",
    "length(used_pids)\n",
    "\n",
    "# pids from img_conc_bscore\n",
    "copy(img_conc_bscore)[, ':='(is_used=pid %in% used_pids)\n",
    "    ][, .(total_pids=.N, used_pids=sum(is_used))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency (Task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# get the object names\n",
    "objdet = read_feather('data/df_objdet.feather') %>% setDT()\n",
    "objdet = objdet[prob>=0.1, .(pid, object=label_name, inst_id, prob)]\n",
    "\n",
    "# get pjson\n",
    "pjson = read_feather('data/pjson.feather') %>% setDT()\n",
    "\n",
    "# get the dictionary\n",
    "freq_dict = read_feather('data/google_freqdict.feather') %>% setDT()\n",
    "freq_dict = freq_dict[, .(word=str_trim(word), freq=freq_google_withoutstop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# get the freq of each object\n",
    "get_freq <- function(obj) {\n",
    "    # obj: string, a single object name\n",
    "    # dict: data.table, bscore dictionary\n",
    "    # output: numeric, the concreteness score of the object name. 0 if not found.\n",
    "    freq_dict[word==obj, fcase(length(freq)==0, 0, length(freq)>0, freq)]\n",
    "}\n",
    "\n",
    "freq = objdet[, \n",
    "    .(pid, inst_id, prob, object=str_extract_all(object, '[A-Za-z]+') %>% lapply(str_trim))\n",
    "    ][, {\n",
    "    # if object name are composite (e.g., brass handhold), we take the mean \n",
    "    freq = sapply(object[[1]], get_freq) %>% mean()  \n",
    "    list(pid, object, prob, inst_id, freq)\n",
    "    },\n",
    "    keyby=seq_len(nrow(objdet))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.table: 1 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>pid</th><th scope=col>freq_sum</th><th scope=col>freq_mean</th><th scope=col>freq_pw_sum</th><th scope=col>freq_pw_mean</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1000117510</td><td>0.9276249</td><td>0.02728309</td><td>0.1880324</td><td>0.02778901</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.table: 1 × 5\n",
       "\\begin{tabular}{lllll}\n",
       " pid & freq\\_sum & freq\\_mean & freq\\_pw\\_sum & freq\\_pw\\_mean\\\\\n",
       " <chr> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1000117510 & 0.9276249 & 0.02728309 & 0.1880324 & 0.02778901\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.table: 1 × 5\n",
       "\n",
       "| pid &lt;chr&gt; | freq_sum &lt;dbl&gt; | freq_mean &lt;dbl&gt; | freq_pw_sum &lt;dbl&gt; | freq_pw_mean &lt;dbl&gt; |\n",
       "|---|---|---|---|---|\n",
       "| 1000117510 | 0.9276249 | 0.02728309 | 0.1880324 | 0.02778901 |\n",
       "\n"
      ],
      "text/plain": [
       "  pid        freq_sum  freq_mean  freq_pw_sum freq_pw_mean\n",
       "1 1000117510 0.9276249 0.02728309 0.1880324   0.02778901  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute the freq\n",
    "img_freq = freq[,\n",
    "    .(freq_sum=sum(freq),  # sum of all objects' bscore\n",
    "      freq_mean=mean(freq),  # mean of all objects' bscore \n",
    "      freq_pw_sum=sum(freq*prob),  # sum of all objects' bscore weighted by prob\n",
    "      freq_pw_mean=sum(freq*prob)/sum(prob)), # mean of all objects' bscore weighted by prob \n",
    "    keyby=.(pid)\n",
    "    ]\n",
    "\n",
    "img_freq[1]\n",
    "fwrite(img_freq, 'data/sharing/img_freq_dict_based.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "3687"
      ],
      "text/latex": [
       "3687"
      ],
      "text/markdown": [
       "3687"
      ],
      "text/plain": [
       "[1] 3687"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_freq[, uniqueN(pid)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (fiver) Image-text Interaction\n",
    "\n",
    "Distance-baesd concreteness. Steps are:\n",
    "- For every PID, Tokenize the image headline. \n",
    "- Compute the embedding of every token in the headline. These embeddings form a cluster.\n",
    "- For every label_name of the PID, compute its embedding. Again, the label_names form another cluster.\n",
    "- Compute the distance:\n",
    "    - Method I: compute the average distance between the clusters\n",
    "    - Method II: compute the shorted distance between the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "import torch\n",
    "import torchtext\n",
    "from pyarrow.feather import write_feather, read_feather\n",
    "\n",
    "from sklearn.metrics import DistanceMetric\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances, manhattan_distances\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "wdir = '/home/yu/OneDrive/Construal'\n",
    "os.chdir(wdir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get emb from title text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize the image titles\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "glove = torchtext.vocab.GloVe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ------ covert text to tokens --------------\n",
    "pjson = read_feather('data/fiver/pjson.feather')\n",
    "\n",
    "titles = pjson.title.to_list()\n",
    "pids = [str(pid) for pid in pjson.pid.to_list()]\n",
    "\n",
    "# get the tokenized title\n",
    "title_tokens = []\n",
    "for i, title in enumerate(nlp.pipe(titles)):\n",
    "    tokens = [t.text for t in title\n",
    "              if t.pos_ in ['NOUN', 'PROPN', 'PRON']]\n",
    "\n",
    "    # if there's no NOUN tokens in the title, we include them all\n",
    "    if len(tokens) == 0:\n",
    "        tokens = [t.text for t in title]\n",
    "    title_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ---------- convert tokens to embs ----------------\n",
    "\n",
    "def get_emb(vocab, tokens):\n",
    "    # get the embedding for every title token\n",
    "    emb_output = {}\n",
    "    for i, (pid, token) in enumerate(zip(pids, tokens)):\n",
    "        emb = []\n",
    "        for t in token:\n",
    "            t = t.lower() if isinstance(vocab, torchtext.vocab.FastText) else t\n",
    "            emb.append(vocab[t].squeeze())\n",
    "        \n",
    "        emb = torch.stack(emb)\n",
    "\n",
    "        emb_output[pid] = emb\n",
    "    return emb_output\n",
    "\n",
    "title_emb_glove = get_emb(glove, title_tokens)\n",
    "\n",
    "torch.save(title_emb_glove, 'data/fiver/image-text/title_emb_glove.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2615"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(title_emb_glove.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get emb from image obj name\n",
    "\n",
    "Here we generate the \"centrioid\" of object name embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>prob</th>\n",
       "      <th>label_id</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000342273</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>27</td>\n",
       "      <td>avocado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pid      prob  label_id label_name\n",
       "0  1000342273  0.001513        27    avocado"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the images classes (R)\n",
    "object_detect_res = read_feather('data/fiver/object-detect-res.feather')\n",
    "\n",
    "# objtxt: the text of image objects, grouped by pid\n",
    "# - Frist we aggreate prob of same labels within a pid\n",
    "# - Then we normalize prob so that the probs of each pid sum to one\n",
    "tmp = object_detect_res\n",
    "objtxt = tmp[(~tmp.label_name.isna()) & (~tmp.prob.isna())] \\\n",
    "    .groupby(['pid', 'label_id']) \\\n",
    "    .agg({'prob': 'sum', 'label_name': 'first'}) \\\n",
    "    .reset_index() \\\n",
    "    .groupby(['pid'], group_keys=True) \\\n",
    "    .apply(lambda x: x.assign(prob=x.prob/x.prob.sum())) \\\n",
    "    .reset_index(drop=True) \\\n",
    "    .loc[:, ['pid', 'prob', 'label_id', 'label_name']] \\\n",
    "    .sort_values(['pid', 'label_id'])\n",
    "\n",
    "# write_feather(objtxt, 'data/fiver/image-text/objtxt.feather')\n",
    "objtxt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize the images objects\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "glove = torchtext.vocab.GloVe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2604/2604 [02:01<00:00, 21.50it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_objname_emb(vocab):\n",
    "    objname_emb = {}\n",
    "    \n",
    "    for pid in tqdm(set(objtxt['pid'].to_list())):\n",
    "        # get embs from object names\n",
    "        objname = objtxt.loc[objtxt.pid==pid, 'label_name'].to_list()  # list of object names for a pid\n",
    "\n",
    "        embs = []\n",
    "        for tokens in nlp.pipe(objname):\n",
    "            tokens = tokens.text.split('_')\n",
    "            embs_temp = []\n",
    "            for t in tokens:\n",
    "                t = t.lower()\n",
    "                embs_temp.append(vocab[t])\n",
    "            embs_temp = torch.stack(embs_temp).mean(0).squeeze()\n",
    "            embs.append(embs_temp)\n",
    "\n",
    "        objname_emb[pid] = torch.stack(embs).numpy() # (N,300)\n",
    "\n",
    "    return objname_emb\n",
    "\n",
    "objname_emb_glove = get_objname_emb(glove)\n",
    "\n",
    "torch.save(objname_emb_glove, 'data/fiver/image-text/objname_emb_glove.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute dist_concreteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# cretae embeddings\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "title_emb_glove = torch.load('data/title_emb_glove.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2604/2604 [01:15<00:00, 34.44it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_distance(text_embs, objname_embs, objtxt):\n",
    "\n",
    "    valid_pids = set(objtxt['pid'].to_list()) \\\n",
    "        .intersection(set(text_embs.keys()))\n",
    "    valid_pids = list(valid_pids)\n",
    "\n",
    "    results = []\n",
    "    for pid in tqdm(valid_pids):\n",
    "        # get embs from text\n",
    "        text_emb = text_embs[pid]  # (n_tokens, dim_emb)\n",
    "        cent_text = text_emb.numpy().mean(0).reshape(1,-1)  # (1, dim_emb)\n",
    "\n",
    "        # get embs from object names\n",
    "        objname_emb = objname_embs[pid]\n",
    "        objname_prob = np.array(objtxt.loc[objtxt.pid==pid, 'prob'].to_list())\n",
    "        cent_objname = np.matmul(objname_prob, objname_emb).reshape(1,-1)\n",
    "\n",
    "        # compute cluster avg distance\n",
    "        linf_dist = DistanceMetric.get_metric('chebyshev')\n",
    "\n",
    "        cluster_cos_dist = cosine_distances(cent_text, cent_objname)[0][0]\n",
    "        cluster_l2_dist = euclidean_distances(cent_text, cent_objname)[0][0]\n",
    "        cluster_l1_dist = manhattan_distances(cent_text, cent_objname)[0][0]\n",
    "        cluster_linf_dist = linf_dist.pairwise(cent_text, cent_objname)[0][0]\n",
    "\n",
    "        # compute shortest pairwise dist\n",
    "        k_pmax = 3\n",
    "        pmax_idx = objname_prob.argsort()[-k_pmax:]\n",
    "        objname_emb = objname_emb[pmax_idx, :]\n",
    "\n",
    "        shortest_cos_dist = cosine_distances(text_emb, objname_emb).min()\n",
    "        shortest_l2_dist = euclidean_distances(text_emb, objname_emb).min()\n",
    "        shortest_l1_dist = manhattan_distances(text_emb, objname_emb).min()\n",
    "        shortest_linf_dist = linf_dist.pairwise(text_emb, objname_emb).min()\n",
    "        \n",
    "        # append to results\n",
    "        results.append((pid, cluster_cos_dist, cluster_l1_dist, cluster_l2_dist, cluster_linf_dist, shortest_cos_dist, shortest_l1_dist, shortest_l2_dist, shortest_linf_dist))\n",
    "\n",
    "    return pd.DataFrame(results, columns=['pid', 'cluster_cos_dist', 'cluster_l1_dist', 'cluster_l2_dist', 'cluster_linf_dist', 'shortest_cos_dist', 'shortest_l1_dist', 'shortest_l2_dist', 'shortest_linf_dist'])\n",
    "\n",
    "    \n",
    "dist_glove_title = get_distance(\n",
    "    title_emb_glove, objname_emb_glove, objtxt)\n",
    "\n",
    "dist_glove_title.to_csv('data/sharing/dist_glove_title.csv')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (fiver) MNI concreteness\n",
    "\n",
    "In this version I used Pandas. pydatatable is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "from pyarrow.feather import read_feather, write_feather\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnext101_32x8d\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, ToPILImage, CenterCrop, RandomResizedCrop, RandomHorizontalFlip\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "wdir = '/home/yu/OneDrive/Construal'\n",
    "os.chdir(wdir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get representation\n",
    "\n",
    "For every project image, use a backbone to generate a representation for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get the `pid` of all projects of all projects\n",
    "pjson = read_feather('data/fiver/pjson.feather')\n",
    "pids = [str(pid) for pid in pjson.pid.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yu/Software/python/python-env/py310-base/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/yu/Software/python/python-env/py310-base/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt101_32X8D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt101_32X8D_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dl)=82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 56/82 [00:35<00:17,  1.44it/s]/home/yu/Software/python/python-env/py310-base/lib/python3.10/site-packages/PIL/Image.py:979: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 82/82 [00:50<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "class ReprDataset(Dataset):\n",
    "    def __init__(self, pids):\n",
    "        self.img_dir = '/home/yu/chaoyang/research-resources/kickstart-raw-from-amrita/fiver-image/Image Folder'\n",
    "\n",
    "        # check all pids exists\n",
    "        valid_pids = []\n",
    "        for pid in pids:\n",
    "            img_path = f'{self.img_dir}/{pid}/{pid}_main.jpg'\n",
    "            try:\n",
    "                with PIL.Image.open(img_path) as img:\n",
    "                    valid_pids.append(pid)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # pids = [pid for pid in pids\n",
    "        #         if os.path.exists(f'{self.img_dir}/{pid}/{pid}_main.jpg')]\n",
    "        self.pids = valid_pids\n",
    "        self.transform = Compose([Resize(256),\n",
    "                                  CenterCrop(224),\n",
    "                                  ToTensor(),\n",
    "                                  Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                            std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid = self.pids[idx]\n",
    "        img_path = f'{self.img_dir}/{pid}/{pid}_main.jpg'\n",
    "        with PIL.Image.open(img_path) as img:\n",
    "            img = img.convert('RGB')\n",
    "            img = self.transform(img)\n",
    "        return pid, img\n",
    "\n",
    "# load and freeze model     \n",
    "model = resnext101_32x8d(pretrained=True)   \n",
    "model.fc = nn.Identity()\n",
    "\n",
    "device = 'cuda:0'\n",
    "model.to(device)\n",
    "\n",
    "# make dataset/dataloader\n",
    "ds = ReprDataset(pids)\n",
    "dl = DataLoader(ds, shuffle=False, batch_size=32, drop_last=False)\n",
    "print(f'{len(dl)=}')\n",
    "results = {}\n",
    "\n",
    "# run!\n",
    "with torch.no_grad():\n",
    "    for i, (pid, img) in enumerate(tqdm(dl)):\n",
    "        img = img.to(device)\n",
    "        img_repr = model(img)\n",
    "\n",
    "        for p, r in zip(pid, img_repr):\n",
    "            results[p] = r\n",
    "\n",
    "torch.save(results, 'data/fiver/mni-concreteness/fiverobj_repr.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get label-level MNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>label_name</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1630789067</td>\n",
       "      <td>suit_(clothing)</td>\n",
       "      <td>1033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pid       label_name  label_id\n",
       "0  1630789067  suit_(clothing)      1033"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_detect_res = read_feather('data/fiver/object-detect-res.feather')\n",
    "project_label_names = object_detect_res \\\n",
    "    .loc[object_detect_res.prob>=0.5, ['pid', 'label_name', 'label_id']] \\\n",
    "    .drop_duplicates()\n",
    "    \n",
    "project_label_names.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load image reprs into an annoy tree\n",
    "reprs = torch.load('data/fiver/mni-concreteness/fiverobj_repr.pt')\n",
    "valid_pids = list(reprs.keys())\n",
    "\n",
    "# load image reprs into an annoy tree\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "t = AnnoyIndex(2048, 'angular')  # Length of item vector that will be indexed\n",
    "\n",
    "pid2id = {}\n",
    "for i, (pid, vec) in enumerate(reprs.items()): \n",
    "    # create a map from label_id to an int\n",
    "    pid2id[pid] = i\n",
    "\n",
    "    # add to annoy tree\n",
    "    vec = vec.cpu().numpy()\n",
    "    t.add_item(i, vec)\n",
    "    \n",
    "t.build(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 445/445 [00:07<00:00, 56.20it/s] \n",
      "100%|██████████| 445/445 [00:09<00:00, 46.13it/s] \n",
      "100%|██████████| 445/445 [00:12<00:00, 36.43it/s] \n",
      "100%|██████████| 445/445 [00:17<00:00, 25.40it/s] \n"
     ]
    }
   ],
   "source": [
    "# run neighbor search\n",
    "text = project_label_names\n",
    "text = text[text.pid.isin(valid_pids)]\n",
    "W = text.label_name.unique().tolist()\n",
    "\n",
    "\n",
    "def make_mni(k, t):\n",
    "    mni_dict = {}\n",
    "\n",
    "    for w in tqdm(W):\n",
    "        # Vw = text[f.label_name==w, [f.pid]]\n",
    "        # Vw = dt.unique(Vw).to_list()[0]\n",
    "\n",
    "        Vw = text[text.label_name==w].pid.unique().tolist()\n",
    "        Vw = [pid2id[pid] for pid in Vw]\n",
    "        Vw = set(Vw)\n",
    "        \n",
    "        a = 0\n",
    "        for v in Vw:\n",
    "            NN_v = set(t.get_nns_by_item(v, k)) - set([v])\n",
    "            a += len(Vw.intersection(NN_v))\n",
    "\n",
    "        mni = a/len(Vw)\n",
    "        adj_mni = mni/(len(Vw)*k)*819\n",
    "\n",
    "        mni_dict[w] = adj_mni\n",
    "\n",
    "    frame = pd.DataFrame({'label': list(mni_dict.keys()), 'mni': list(mni_dict.values())})\n",
    "    frame = frame.sort_values('mni', ascending=False)\n",
    "\n",
    "    return frame\n",
    "\n",
    "dt_k10 = make_mni(10, t)\n",
    "dt_k25 = make_mni(25, t)\n",
    "dt_k50 = make_mni(50, t)\n",
    "dt_k100 = make_mni(100, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# merge different neighbor sizes\n",
    "mni = dt_k10.merge(dt_k25, on='label', suffixes=('_k10', '_k25')) \\\n",
    "    .merge(dt_k50, on='label', suffixes=('', '_k50')) \\\n",
    "    .merge(dt_k100, on='label', suffixes=('_k50', '_k100'))\n",
    "\n",
    "write_feather(mni, 'data/fiver/mni-concreteness/mni.feather')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get pid-level MNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.table: 1 × 9</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>pid</th><th scope=col>mni_k10_weighted_unnormalized</th><th scope=col>mni_k10_weighted_normalized</th><th scope=col>mni_k25_weighted_unnormalized</th><th scope=col>mni_k25_weighted_normalized</th><th scope=col>mni_k50_weighted_unnormalized</th><th scope=col>mni_k50_weighted_normalized</th><th scope=col>mni_k100_weighted_unnormalized</th><th scope=col>mni_k100_weighted_normalized</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1000342273</td><td>12.02429</td><td>1.864386</td><td>10.59487</td><td>1.642752</td><td>9.425896</td><td>1.461501</td><td>7.652324</td><td>1.186506</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.table: 1 × 9\n",
       "\\begin{tabular}{lllllllll}\n",
       " pid & mni\\_k10\\_weighted\\_unnormalized & mni\\_k10\\_weighted\\_normalized & mni\\_k25\\_weighted\\_unnormalized & mni\\_k25\\_weighted\\_normalized & mni\\_k50\\_weighted\\_unnormalized & mni\\_k50\\_weighted\\_normalized & mni\\_k100\\_weighted\\_unnormalized & mni\\_k100\\_weighted\\_normalized\\\\\n",
       " <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 1000342273 & 12.02429 & 1.864386 & 10.59487 & 1.642752 & 9.425896 & 1.461501 & 7.652324 & 1.186506\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.table: 1 × 9\n",
       "\n",
       "| pid &lt;chr&gt; | mni_k10_weighted_unnormalized &lt;dbl&gt; | mni_k10_weighted_normalized &lt;dbl&gt; | mni_k25_weighted_unnormalized &lt;dbl&gt; | mni_k25_weighted_normalized &lt;dbl&gt; | mni_k50_weighted_unnormalized &lt;dbl&gt; | mni_k50_weighted_normalized &lt;dbl&gt; | mni_k100_weighted_unnormalized &lt;dbl&gt; | mni_k100_weighted_normalized &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| 1000342273 | 12.02429 | 1.864386 | 10.59487 | 1.642752 | 9.425896 | 1.461501 | 7.652324 | 1.186506 |\n",
       "\n"
      ],
      "text/plain": [
       "  pid        mni_k10_weighted_unnormalized mni_k10_weighted_normalized\n",
       "1 1000342273 12.02429                      1.864386                   \n",
       "  mni_k25_weighted_unnormalized mni_k25_weighted_normalized\n",
       "1 10.59487                      1.642752                   \n",
       "  mni_k50_weighted_unnormalized mni_k50_weighted_normalized\n",
       "1 9.425896                      1.461501                   \n",
       "  mni_k100_weighted_unnormalized mni_k100_weighted_normalized\n",
       "1 7.652324                       1.186506                    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suppressMessages({\n",
    "    library(arrow)\n",
    "    library(data.table)\n",
    "})\n",
    "\n",
    "wdir = '/home/yu/OneDrive/Construal'\n",
    "setwd(wdir)\n",
    "\n",
    "object_detect_res = read_feather(\"data/fiver/object-detect-res.feather\") %>% as.data.table()\n",
    "mni = read_feather('data/fiver/mni-concreteness/mni.feather') %>% as.data.table()\n",
    "\n",
    "pid_mni_weighted = object_detect_res[, .(pid, label_name, inst_id, prob)\n",
    "    ][mni, on=c('label_name==label'), nomatch=NULL\n",
    "    ][, .(mni_k10_weighted_unnormalized=sum(prob*mni_k10),\n",
    "          mni_k10_weighted_normalized=sum(prob*mni_k10)/sum(prob),\n",
    "          mni_k25_weighted_unnormalized=sum(prob*mni_k25),\n",
    "          mni_k25_weighted_normalized=sum(prob*mni_k25)/sum(prob),\n",
    "          mni_k50_weighted_unnormalized=sum(prob*mni_k50),\n",
    "          mni_k50_weighted_normalized=sum(prob*mni_k50)/sum(prob),\n",
    "          mni_k100_weighted_unnormalized=sum(prob*mni_k100),\n",
    "          mni_k100_weighted_normalized=sum(prob*mni_k100)/sum(prob)),\n",
    "      keyby=.(pid)\n",
    "    ][order(pid)]\n",
    "\n",
    "pid_mni_weighted[1]\n",
    "fwrite(pid_mni_weighted, 'data/fiver/mni-concreteness/pid_mni_weighted.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
